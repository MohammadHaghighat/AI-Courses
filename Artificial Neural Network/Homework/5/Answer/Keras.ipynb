{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJe9OjQ4XNBZ"
   },
   "source": [
    "**You must complete the cells with ### Your code goes here ###**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVEKl-VoTVh7"
   },
   "source": [
    "\n",
    "## **Transfer Learning**\n",
    "Transfer learning in Convolutional Neural Networks (CNNs) is a technique where a model developed for a specific task is reused as the starting point for a model on a second, related task. This approach leverages the knowledge gained from the original task to improve the performance and training efficiency of the new task. Here’s how it works and why it’s useful:\n",
    "\n",
    "\n",
    "**How Transfer Learning Works**\n",
    "\n",
    "**Pre-trained Model:**\n",
    "\n",
    "A CNN is first trained on a large dataset for a task such as image classification. Popular pre-trained models include VGG, ResNet, and Inception, which are trained on large datasets like ImageNet.\n",
    "\n",
    "**Feature Extraction:**\n",
    "\n",
    "The pre-trained model has already learned useful features from the initial dataset. For transfer learning, we use the weights and architecture of this pre-trained model and repurpose it for a new, related task.\n",
    "\n",
    "**Fine-tuning:**\n",
    "\n",
    "The final layers of the pre-trained model are either replaced or appended with new layers that are specific to the new task. Only these new layers, or sometimes the entire network, are retrained on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k2Vuq7uHSEYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Conv2D, Flatten, Activation, MaxPool2D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqmYtApQUrVd"
   },
   "source": [
    "**We load the cifar 10 dataset then separate the first 5 classes of data as the first training set.**\n",
    "**Images with labels less than 5 are put in set 1, and the rest are put in set 2. Data is also normalized by dividing by 255.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F7atiP0jTflt"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "n_examples = 50000\n",
    "\n",
    "\n",
    "X1_train = []\n",
    "X1_test = []\n",
    "X2_train = []\n",
    "X2_test = []\n",
    "Y1_train = []\n",
    "Y1_test = []\n",
    "Y2_train = []\n",
    "Y2_test = []\n",
    "\n",
    "for ix in range(n_examples):\n",
    "    if y_train[ix] < 5:\n",
    "        # put data in set 1\n",
    "        X1_train.append(x_train[ix]/255.0) #Dividing by 255 is for normalization\n",
    "        Y1_train.append(y_train[ix])\n",
    "    else:\n",
    "        # put data in set 2\n",
    "        X2_train.append(x_train[ix]/255.0)\n",
    "        Y2_train.append(y_train[ix])\n",
    "\n",
    "for ix in range(y_test.shape[0]):\n",
    "    if y_test[ix] < 5:\n",
    "        # put data in set 1\n",
    "        X1_test.append(x_test[ix]/255.0)\n",
    "        Y1_test.append(y_test[ix])\n",
    "    else:\n",
    "        # put data in set 2\n",
    "        X2_test.append(x_test[ix]/255.0)\n",
    "        Y2_test.append(y_test[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D-kjb9QVPkp"
   },
   "source": [
    "**Convert the lists to NumPy arrays and reshape them to the correct input format. Labels are one-hot encoded for classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7enHxPaTh4E",
    "outputId": "7a724499-3623-4d7b-f6b5-94d22a6c4833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "(25000, 5) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "X1_train = np.asarray(X1_train).reshape((-1, 32, 32, 3))\n",
    "X1_test = np.asarray(X1_test).reshape((-1, 32, 32, 3))\n",
    "X2_train = np.asarray(X2_train).reshape((-1, 32, 32, 3))\n",
    "X2_test = np.asarray(X2_test).reshape((-1, 32, 32, 3))\n",
    "\n",
    "Y1_train = to_categorical(np.asarray(Y1_train), 5)\n",
    "Y1_test = to_categorical(np.asarray(Y1_test), 5)\n",
    "\n",
    "Y2_train = to_categorical(np.asarray(Y2_train), 10)\n",
    "Y2_test = to_categorical(np.asarray(Y2_test), 10)\n",
    "print (X1_train.shape, X1_test.shape)\n",
    "print (Y1_train.shape, Y1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcSsZ7smWtCq"
   },
   "source": [
    "**Split the training data into training and validation sets using an 80-20 split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2V2LE8x4TpDU"
   },
   "outputs": [],
   "source": [
    "split1 = int(0.8 * X1_train.shape[0])\n",
    "split2 = int(0.8 * X2_train.shape[0])\n",
    "\n",
    "x1_val = X1_train[split1:]\n",
    "x1_train = X1_train[:split1]\n",
    "y1_val = Y1_train[split1:]\n",
    "y1_train = Y1_train[:split1]\n",
    "\n",
    "x2_val = X2_train[split2:]\n",
    "x2_train = X2_train[:split2]\n",
    "y2_val = Y2_train[split2:]\n",
    "y2_train = Y2_train[:split2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QBp4XrbWxEi"
   },
   "source": [
    "**Build and compile Keras CNN model for classification.**\n",
    "\n",
    "Your layers should be arranged like this:\n",
    "\n",
    "Layer 1. Convolutional layer with 32 4x4 kernels, expecting input of shape (32, 32, 3) and using relu as activation function.\n",
    "\n",
    "Layer 2. 2DMaxPooling layer  which pool_size is 2x2.\n",
    "\n",
    "Layer 3. Convolutional layer with 32 4x4 kernels, expecting input of shape (32, 32, 3) and using relu as activation function.\n",
    "\n",
    "Layer 4. 2DMaxPooling layer  which pool_size is 2x2.\n",
    "\n",
    "Layer 5. Flatten layer\n",
    "\n",
    "\n",
    "Layer 7. Fully connected layer with 256 neurons, using relu as activation function.\n",
    "\n",
    "Layer 8. Fully connected layer with 5 neurons - acting as the final output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5tLyxr1jWyON"
   },
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(4, 4), input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(4, 4), input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wAlC2arZnvh"
   },
   "source": [
    "**Now it's time to train the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO_Uh4MJTt7J",
    "outputId": "990c1d40-3c8b-4778-e77a-d78651e57aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 - 21s - 105ms/step - accuracy: 0.5411 - loss: 1.1082 - val_accuracy: 0.6202 - val_loss: 0.9377\n",
      "Epoch 2/10\n",
      "200/200 - 22s - 109ms/step - accuracy: 0.6431 - loss: 0.8935 - val_accuracy: 0.6688 - val_loss: 0.8527\n",
      "Epoch 3/10\n",
      "200/200 - 42s - 208ms/step - accuracy: 0.6780 - loss: 0.8194 - val_accuracy: 0.6874 - val_loss: 0.8061\n",
      "Epoch 4/10\n",
      "200/200 - 39s - 194ms/step - accuracy: 0.7090 - loss: 0.7501 - val_accuracy: 0.7110 - val_loss: 0.7498\n",
      "Epoch 5/10\n",
      "200/200 - 22s - 110ms/step - accuracy: 0.7297 - loss: 0.6993 - val_accuracy: 0.7038 - val_loss: 0.7529\n",
      "Epoch 6/10\n",
      "200/200 - 41s - 206ms/step - accuracy: 0.7481 - loss: 0.6560 - val_accuracy: 0.7250 - val_loss: 0.7205\n",
      "Epoch 7/10\n",
      "200/200 - 20s - 101ms/step - accuracy: 0.7589 - loss: 0.6311 - val_accuracy: 0.7166 - val_loss: 0.7578\n",
      "Epoch 8/10\n",
      "200/200 - 22s - 109ms/step - accuracy: 0.7844 - loss: 0.5715 - val_accuracy: 0.7488 - val_loss: 0.6856\n",
      "Epoch 9/10\n",
      "200/200 - 20s - 98ms/step - accuracy: 0.7976 - loss: 0.5365 - val_accuracy: 0.7540 - val_loss: 0.6449\n",
      "Epoch 10/10\n",
      "200/200 - 21s - 105ms/step - accuracy: 0.8098 - loss: 0.5038 - val_accuracy: 0.7448 - val_loss: 0.6883\n",
      "\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Time taken for first training:  0:04:29.738770\n",
      "\n",
      " -------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "hist1 = model.fit(x1_train, y1_train,\n",
    "         epochs=10,\n",
    "         shuffle=True,\n",
    "         batch_size=100,\n",
    "         validation_data=(x1_val, y1_val), verbose=2)\n",
    "\n",
    "time_taken = datetime.datetime.now() - start\n",
    "print ('\\n'*2, '-'*20, '\\n')\n",
    "print ('Time taken for first training: ', time_taken)\n",
    "print ('\\n', '-'*20, '\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MFt13cTaqkW"
   },
   "source": [
    "**The convolutional neural network for training a model for classes 5..9 by using the information gained by previous model is as follows:**\n",
    "\n",
    "\n",
    "**Build and compile Follwed Keras CNN model for classification.**\n",
    "\n",
    "Your layers should be arranged like this:\n",
    "\n",
    "Layer 1-5: First 5 layers of the previous model (basically the convoluted layers) are used as it is in this model. Further they are made non-trainable to preserve the information gained by them while training of previous model. (So that means sense to freeze them.)\n",
    "\n",
    "Layer 7: Fully connected layer with 128 neurons, using relu as activation function.\n",
    "\n",
    "Layer 8. Fully connected layer with 10 neurons - acting as the final output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bj25oIQfT1Oq"
   },
   "outputs": [],
   "source": [
    "previous_model = model\n",
    "new_model = Sequential()\n",
    "for layer in previous_model.layers[:5]:\n",
    "    layer.trainable = False\n",
    "    new_model.add(layer)\n",
    "\n",
    "new_model.add(Dense(128, activation='relu'))\n",
    "new_model.add(Dense(10, activation='softmax'))\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR1GoSlRaFq1"
   },
   "source": [
    "**Now it's time to train the 2nd Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93K26RV_T1bW",
    "outputId": "10c9120c-f157-4bf8-aa05-4119e0f7c0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 - 8s - 38ms/step - accuracy: 0.7272 - loss: 0.7558 - val_accuracy: 0.7824 - val_loss: 0.5955\n",
      "Epoch 2/10\n",
      "200/200 - 9s - 46ms/step - accuracy: 0.7915 - loss: 0.5699 - val_accuracy: 0.7904 - val_loss: 0.5783\n",
      "Epoch 3/10\n",
      "200/200 - 11s - 53ms/step - accuracy: 0.8077 - loss: 0.5234 - val_accuracy: 0.7978 - val_loss: 0.5491\n",
      "Epoch 4/10\n",
      "200/200 - 11s - 55ms/step - accuracy: 0.8163 - loss: 0.5002 - val_accuracy: 0.8062 - val_loss: 0.5349\n",
      "Epoch 5/10\n",
      "200/200 - 11s - 53ms/step - accuracy: 0.8262 - loss: 0.4737 - val_accuracy: 0.8122 - val_loss: 0.5178\n",
      "Epoch 6/10\n",
      "200/200 - 6s - 28ms/step - accuracy: 0.8348 - loss: 0.4523 - val_accuracy: 0.8176 - val_loss: 0.5025\n",
      "Epoch 7/10\n",
      "200/200 - 10s - 51ms/step - accuracy: 0.8417 - loss: 0.4358 - val_accuracy: 0.8176 - val_loss: 0.5050\n",
      "Epoch 8/10\n",
      "200/200 - 11s - 56ms/step - accuracy: 0.8469 - loss: 0.4178 - val_accuracy: 0.8252 - val_loss: 0.4984\n",
      "Epoch 9/10\n",
      "200/200 - 11s - 56ms/step - accuracy: 0.8577 - loss: 0.3963 - val_accuracy: 0.8264 - val_loss: 0.4892\n",
      "Epoch 10/10\n",
      "200/200 - 9s - 43ms/step - accuracy: 0.8629 - loss: 0.3791 - val_accuracy: 0.8234 - val_loss: 0.4992\n",
      "\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Time taken for final training:  0:01:36.176614\n",
      "\n",
      " -------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_model = new_model\n",
    "start = datetime.datetime.now()\n",
    "hist2 = trans_model.fit(x2_train, y2_train, epochs=10, shuffle=True, batch_size=100, validation_data=(x2_val, y2_val), verbose=2)\n",
    "time_taken = datetime.datetime.now() - start\n",
    "print ('\\n'*2, '-'*20, '\\n')\n",
    "print ('Time taken for final training: ', time_taken)\n",
    "print ('\\n', '-'*20, '\\n'*2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
