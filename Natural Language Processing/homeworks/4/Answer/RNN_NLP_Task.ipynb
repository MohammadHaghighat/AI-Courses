{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzPFDjvJG86p"
      },
      "source": [
        "### Detailed Description of the Problem (Topic Detection in Reviews)\n",
        "\n",
        "#### **Objective**\n",
        "The goal of this task is to identify the topics of customer reviews related to products sold on the Digikala platform. These reviews are categorized based on various product features, such as price, quality, warranty, and more. The challenge is to determine whether each review is associated with specific topics or attributes of the product.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Data Structure**\n",
        "The provided dataset contains various features and annotations for each review, which can be utilized for analysis and topic extraction. Each review includes the following fields:\n",
        "\n",
        "1. **`id`**: A unique identifier for the review.\n",
        "2. **`comment`**: The text of the review submitted by the customer.\n",
        "3. **`product_id`**: A unique identifier for the product related to the review.\n",
        "4. **`product_title_fa`**: The product title in Persian.\n",
        "5. **`category_id`**: A unique identifier for the category of the product.\n",
        "6. **`category_title_fa`**: The category title in Persian.\n",
        "7. **`is_buyer`**: Indicates whether the review was written by a verified buyer of the product.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Additional Features**\n",
        "The dataset also includes binary labels (0 or 1) for various product attributes. These labels indicate whether the review is associated with specific product-related topics. The additional features are as follows:\n",
        "\n",
        "1. **`price_value`**: Whether the review discusses the value or fairness of the product's price.\n",
        "2. **`fake_originality`**: Whether the review discusses concerns about the product's originality or authenticity.\n",
        "3. **`warranty`**: Whether the review mentions after-sales services or warranty-related issues.\n",
        "4. **`size`**: Whether the review discusses the size or dimensions of the product.\n",
        "5. **`discrepancy`**: Whether the review mentions any discrepancies between the product description and the actual product received.\n",
        "6. **`flavor_odor`**: Whether the review discusses the flavor or odor of the product (applicable to consumables or similar items).\n",
        "7. **`expiration_date`**: Whether the review mentions the expiration date of the product (applicable to perishable goods).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Additional Information**\n",
        "- These labels are intended to facilitate the analysis and extraction of topics from customer reviews.\n",
        "- Each topic (such as price or quality) is represented as a binary value (`0` or `1`):\n",
        "  - **`1`**: Indicates that the review is associated with the specific topic.\n",
        "  - **`0`**: Indicates that the review does not mention the specific topic.\n",
        "\n",
        "The task is essentially a **multi-label classification problem**, where a single review can be associated with multiple topics simultaneously. For instance, a review might discuss both the **price** and **warranty** of a product, but not its **size** or **expiration date**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Objective Summary**\n",
        "The main objective is to train a model that can:\n",
        "1. Process the text of the reviews (along with other features if necessary).\n",
        "2. Identify which topics (labels) are relevant for each review.\n",
        "\n",
        "This is useful for:\n",
        "- Automating the analysis of customer feedback.\n",
        "- Providing insights into common customer concerns about products.\n",
        "- Enhancing product recommendations and quality control processes.\n",
        "\n",
        "\n",
        "Based on this description, train a good RNN model to solve this problem.\n",
        "\n",
        "Please train SimpleRNN, GRU and LSTM models and compare their performances.\n",
        "\n",
        "Please adjust the hyperparameters based on your experiments to reach out best performances. (Don't rely on default values)\n",
        "\n",
        "Please fill the '...' in the following code.\n",
        "\n",
        "Download the Dataset from this [Link](https://drive.google.com/file/d/1QOcw01rxMIkJyl2oDEL1mlJYnIEAtDTr/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWDUvzbNyt7j",
        "outputId": "6ba23800-2f70-45a5-e975-47b9110f19a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.9.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.5.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.1.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSRMsoRlyfGH",
        "outputId": "4486b514-d96e-44f8-a5b8-7c14f476aa62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training SimpleRNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 220.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.09661210470066421, Train F1: 0.37141615424365887\n",
            "Epoch 1/10, Val Loss: 0.07439315161493723, Val F1: 0.4610352055886427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 222.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 0.07305584182972082, Train F1: 0.4880067431145138\n",
            "Epoch 2/10, Val Loss: 0.06736496037746462, Val F1: 0.5065407925755172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 221.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 0.06682406344998104, Train F1: 0.54600091491648\n",
            "Epoch 3/10, Val Loss: 0.06474280892771678, Val F1: 0.5205499527654932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 220.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 0.06283673863235774, Train F1: 0.6092341146437168\n",
            "Epoch 4/10, Val Loss: 0.06303244833429257, Val F1: 0.5886910348158076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 221.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 0.059995291502661016, Train F1: 0.6418802931192139\n",
            "Epoch 5/10, Val Loss: 0.062121736305565195, Val F1: 0.6462513593620295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 212.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.058513152214308285, Train F1: 0.6653598754104156\n",
            "Epoch 6/10, Val Loss: 0.06180254706897681, Val F1: 0.6272738866970925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 214.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 0.05608997342783657, Train F1: 0.6762852794217135\n",
            "Epoch 7/10, Val Loss: 0.061606788699556346, Val F1: 0.6119955603751678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 219.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 0.054773784102306974, Train F1: 0.7028168750959859\n",
            "Epoch 8/10, Val Loss: 0.060918421071346915, Val F1: 0.6596929681584783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 219.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 0.053452545401376325, Train F1: 0.7213841483237264\n",
            "Epoch 9/10, Val Loss: 0.06276448994088445, Val F1: 0.673005299366294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:12<00:00, 222.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 0.052219075438627, Train F1: 0.7355769043093471\n",
            "Epoch 10/10, Val Loss: 0.06250936375896363, Val F1: 0.690785486788952\n",
            "\n",
            "Training GRU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 206.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.0802555855484593, Train F1: 0.5087159277633225\n",
            "Epoch 1/10, Val Loss: 0.05913869043368416, Val F1: 0.6674925360968371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 202.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 0.05826961445348931, Train F1: 0.7286282546252687\n",
            "Epoch 2/10, Val Loss: 0.055165813635296046, Val F1: 0.7338188228606713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 205.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 0.053187685938542714, Train F1: 0.7660574392660094\n",
            "Epoch 3/10, Val Loss: 0.05475560350665097, Val F1: 0.7604820623991068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 207.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 0.049481756500913084, Train F1: 0.7915278655043556\n",
            "Epoch 4/10, Val Loss: 0.055463121964804796, Val F1: 0.7560710725406433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 203.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 0.04621113394470161, Train F1: 0.815012758250875\n",
            "Epoch 5/10, Val Loss: 0.057059924565946496, Val F1: 0.7656721853406022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 208.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.0430829816841541, Train F1: 0.8328699004253816\n",
            "Epoch 6/10, Val Loss: 0.058244712953688244, Val F1: 0.749586237623993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 205.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 0.04010744272052529, Train F1: 0.8495238756870073\n",
            "Epoch 7/10, Val Loss: 0.06009359280718291, Val F1: 0.7533841402370911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 204.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 0.037150966392751575, Train F1: 0.8689026752877889\n",
            "Epoch 8/10, Val Loss: 0.06280235960526916, Val F1: 0.7600853466909664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 206.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 0.03457561682651802, Train F1: 0.8822301383403511\n",
            "Epoch 9/10, Val Loss: 0.06522025748524023, Val F1: 0.7586554159977084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 207.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 0.03235156361800057, Train F1: 0.8917873137037917\n",
            "Epoch 10/10, Val Loss: 0.06744814779046131, Val F1: 0.7500603031125992\n",
            "\n",
            "Training LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 200.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.08777200605329405, Train F1: 0.4120829244991367\n",
            "Epoch 1/10, Val Loss: 0.06280120749414478, Val F1: 0.5041659468408394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:14<00:00, 198.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 0.06049640016094499, Train F1: 0.6357658635633039\n",
            "Epoch 2/10, Val Loss: 0.05718766626889968, Val F1: 0.6847698991603394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 200.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 0.05448540471837212, Train F1: 0.731822219111005\n",
            "Epoch 3/10, Val Loss: 0.05654359110943261, Val F1: 0.7284021518478238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:14<00:00, 196.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 0.05072663937376907, Train F1: 0.7747754373806292\n",
            "Epoch 4/10, Val Loss: 0.055801178842846744, Val F1: 0.7376260308944961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:14<00:00, 199.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 0.04740983147545053, Train F1: 0.7959453679534958\n",
            "Epoch 5/10, Val Loss: 0.05595968348823073, Val F1: 0.759336111954721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 201.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.04438560457666173, Train F1: 0.8157288512402187\n",
            "Epoch 6/10, Val Loss: 0.05779246886934389, Val F1: 0.7582640071702335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 200.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 0.04186112995323945, Train F1: 0.8248381695321614\n",
            "Epoch 7/10, Val Loss: 0.058813617082469474, Val F1: 0.7415537601889873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:13<00:00, 201.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 0.03949050213623098, Train F1: 0.8442827325372935\n",
            "Epoch 8/10, Val Loss: 0.060437245073910274, Val F1: 0.7534200790620821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:14<00:00, 197.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 0.037327788458428456, Train F1: 0.8560997782223991\n",
            "Epoch 9/10, Val Loss: 0.06138204489588823, Val F1: 0.7580518799450505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2802/2802 [00:14<00:00, 196.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 0.03541036680683781, Train F1: 0.8646629758022378\n",
            "Epoch 10/10, Val Loss: 0.06336758097104951, Val F1: 0.7557772547266212\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from hazm import Normalizer, word_tokenize, Stemmer, Lemmatizer, stopwords_list\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Preprocessing for Persian Text\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "stopwords = set(stopwords_list())\n",
        "\n",
        "def preprocess_text(text):\n",
        "    '''\n",
        "    Do the pre-process steps as needed.\n",
        "    For your choices, do experiments.\n",
        "    '''\n",
        "    text = normalizer.normalize(text)  # Normalize\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    clean_tokens = [] # Clean tokens\n",
        "    for token in tokens:\n",
        "       if token not in stopwords:\n",
        "        clean_tokens.append(lemmatizer.lemmatize(stemmer.stem(token)))\n",
        "    tokens = ' '.join(clean_tokens)\n",
        "    return tokens\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Handle missing and non-string comments\n",
        "train_data['comment'] = train_data['comment'].fillna('').astype(str)\n",
        "\n",
        "# Preprocess the comments\n",
        "train_data['cleaned_comment'] = train_data['comment'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(texts, max_vocab_size=20000):\n",
        "    '''\n",
        "    The function builds a vocabulary (a mapping of words to unique integer indices) from a list of text samples.\n",
        "    It includes the most frequent words up to a specified maximum vocabulary size.\n",
        "    '''\n",
        "    # Count Word Frequencies\n",
        "    word_counter = Counter()\n",
        "    for text in texts:\n",
        "      for word in text.split():\n",
        "            word_counter[word] += 1\n",
        "    # Create Vocabulary\n",
        "    most_common_words = word_counter.most_common(max_vocab_size - 1)\n",
        "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(most_common_words)}\n",
        "    vocab['<PAD>'] = 0\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_data['cleaned_comment'])\n",
        "max_len = 100\n",
        "\n",
        "def text_to_sequence(text, vocab, max_len):\n",
        "    '''\n",
        "    The function converts a single piece of text into a numerical sequence of fixed length.\n",
        "    Each word in the text is mapped to its corresponding integer index from a vocabulary (vocab).\n",
        "    Words not in the vocabulary are assigned a default value (0).\n",
        "    '''\n",
        "    tokens = text.split()\n",
        "    sequence = [vocab.get(word, 0) for word in tokens] # Word-to-Index Mapping\n",
        "    if len(sequence) > max_len:\n",
        "        sequence = sequence[:max_len]\n",
        "    else:\n",
        "        sequence.extend([0] * (max_len - len(sequence)))\n",
        "\n",
        "    return sequence # Padding or Truncating\n",
        "\n",
        "max_len = 100\n",
        "train_data['sequence'] = train_data['cleaned_comment'].apply(lambda x: text_to_sequence(x, vocab, max_len))\n",
        "\n",
        "# Targets\n",
        "X = np.array(train_data['sequence'].tolist())\n",
        "y = train_data[['price_value', 'fake_originality', 'warranty', 'size', 'discrepancy', 'flavor_odor', 'expiration_date']].values  # Assuming target columns are after column 2\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset = TextDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model Architecture\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, rnn_type='SimpleRNN'):\n",
        "        '''\n",
        "        if needed, change the arguments\n",
        "        '''\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size) # Define embedding layer\n",
        "\n",
        "        bidirectional = True\n",
        "        if rnn_type == 'SimpleRNN':\n",
        "\n",
        "            self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, bidirectional=bidirectional) # Define your own RNN layer\n",
        "\n",
        "        elif rnn_type == 'GRU':\n",
        "\n",
        "            self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True, bidirectional=bidirectional) # Define your own GRU layer\n",
        "\n",
        "        elif rnn_type == 'LSTM':\n",
        "\n",
        "            self.rnn = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=bidirectional) # Define your own LSTM layer\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), output_size) # Define your own Linear layer\n",
        "        self.dropout = nn.Dropout(0.3) # Define your own Dropout layer\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        embed -> r-nn -> dropout -> fc\n",
        "        '''\n",
        "        x = self.embedding(x)\n",
        "        if isinstance(self.rnn, nn.LSTM):\n",
        "            _, (hidden, _) = self.rnn(x)\n",
        "        else:\n",
        "            _, hidden = self.rnn(x)\n",
        "\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.rnn.bidirectional else hidden[-1]\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(hidden)\n",
        "        return x\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# Function to compute F1 score\n",
        "def compute_f1_score(y_true, y_pred):\n",
        "    # Binarize predictions using a threshold of 0.5\n",
        "    binarizer = Binarizer(threshold=0.5)\n",
        "    y_pred_binarized = binarizer.transform(y_pred)\n",
        "    return f1_score(y_true, y_pred_binarized, average='macro')\n",
        "\n",
        "# Training function with F1 score and NaN handling\n",
        "def train_model_with_f1(rnn_type, vocab_size, embed_size, hidden_size, output_size, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = RNNModel(vocab_size, embed_size, hidden_size, output_size, rnn_type=rnn_type).to(device) # Define your model\n",
        "    criterion = nn.BCEWithLogitsLoss() # Define your own criterion w.r.t. problem\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001) # Define your own optimizer based on your choice\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        all_train_preds = []\n",
        "        all_train_targets = []\n",
        "        for X_batch, y_batch in tqdm(train_loader):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device) # transfer to the device\n",
        "\n",
        "            # Handle NaN in y_batch\n",
        "            y_batch = torch.nan_to_num(y_batch, nan=0.0)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Feedforward pass\n",
        "            outputs = model(X_batch)\n",
        "            # compute loss\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            # Do back propagation\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and targets for F1 score\n",
        "            all_train_preds.append(outputs.detach().cpu().numpy())\n",
        "            all_train_targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "        # Compute training F1 score\n",
        "        train_preds = np.vstack(all_train_preds)\n",
        "        train_targets = np.vstack(all_train_targets)\n",
        "        train_f1 = compute_f1_score(train_targets, torch.sigmoid(torch.tensor(train_preds)).numpy())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_preds = []\n",
        "        all_val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                # Handle NaN in y_batch\n",
        "                y_batch = torch.nan_to_num(y_batch, nan=0.0)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Collect predictions and targets for F1 score\n",
        "                all_val_preds.append(outputs.cpu().numpy())\n",
        "                all_val_targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "        # Compute validation F1 score\n",
        "        val_preds = np.vstack(all_val_preds)\n",
        "        val_targets = np.vstack(all_val_targets)\n",
        "        val_f1 = compute_f1_score(val_targets, torch.sigmoid(torch.tensor(val_preds)).numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader)}, Train F1: {train_f1}\")\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss / len(val_loader)}, Val F1: {val_f1}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f\"best_{rnn_type}_model.pth\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Compare Models\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 128\n",
        "hidden_size = 64\n",
        "output_size = y.shape[1]\n",
        "\n",
        "for rnn_type in ['SimpleRNN', 'GRU', 'LSTM']:\n",
        "    print()\n",
        "    print(f\"Training {rnn_type}...\")\n",
        "    train_model_with_f1(rnn_type, vocab_size, embed_size, hidden_size, output_size)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}