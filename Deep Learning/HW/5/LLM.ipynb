{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://logoyab.com/wp-content/uploads/2024/08/IUST-University-Logo-1030x1030.png\" alt=\"Logo\" width=\"200\">\n",
        "    <p><b>HW5 @ Deep Learning Course, Dr. Mohammadi</b></p>\n",
        "    <p><b>ِDesinged by Nafiseh Ahmadi</b></p>\n",
        "</div>\n",
        "\n",
        "--------\n"
      ],
      "metadata": {
        "id": "2YnI5jQP6i3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Full Name:*\n",
        "\n",
        "*Student Number:*\n",
        "\n",
        "\n",
        "------\n"
      ],
      "metadata": {
        "id": "XXmbE_pE6hyr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfrgatGB3Aac"
      },
      "source": [
        "# What are Soft prompts?\n",
        "Soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset; the downside is that they aren’t human readable because you aren’t matching these “virtual tokens” to the embeddings of a real word.\n",
        "<br>\n",
        "<div>\n",
        "<img src=\"https://www.researchgate.net/publication/366062946/figure/fig1/AS:11431281105340756@1670383256990/The-comparison-between-the-previous-T5-prompt-tuning-method-part-a-and-the-introduced.jpg\">\n",
        "</div>\n",
        "\n",
        "Read More:\n",
        "<br>[Youtube : PEFT and Soft Prompt](https://www.youtube.com/watch?v=8uy_WII76L0)\n",
        "<br>[Paper: The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
        "https://arxiv.org/pdf/2101.00190.pdf\n",
        "<br>[Paper: Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLaYIRN4X7cC"
      },
      "source": [
        "# Part 1\n",
        "Before diving into the practical applications, let's first ensure your foundational knowledge is solid. Please answer the following questions.\n",
        "\n",
        "\n",
        "**A) Compare and contrast model tuning and prompt tuning in terms of their effectiveness for specific downstream tasks.**\n",
        "\n",
        "**B) Explore the challenges associated with interpreting soft prompts in the continuous embedding space and propose potential solutions.**\n",
        "\n",
        "**C) What is the effect of initializing prompts randomly versus initializing them from the vocabulary, and how does this impact the performance of prompt tuning?**\n",
        "\n",
        "**D) How is the optimization process in the prefix tuning(<br>[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)) and Why did they use this technique?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9CqqGdLMpvO"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>A :</b></font>"
      ],
      "metadata": {
        "id": "nCEZTUl18cy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>B :</b></font>"
      ],
      "metadata": {
        "id": "Wn9lBBUY8i7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>C :</b></font>"
      ],
      "metadata": {
        "id": "NP__pgyy8iyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>D :</b></font>"
      ],
      "metadata": {
        "id": "_0B4ZZ6Z8isS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQMgi75DV-bo"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3gjmZROpLP5"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG_yY6ep9C7S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_UcfBmLpRiJ"
      },
      "source": [
        "## Model Selection & Constants\n",
        "We will use `bert-fa-base-uncased` as our base model from Hugging Face ([HF_Link](https://huggingface.co/HooshvareLab/bert-fa-base-uncased)). For our tuning, we intend to utilize 20 soft prompt tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV2aP8bwV-z5"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "    seed =\n",
        "    max_len =\n",
        "    train_batch =\n",
        "    valid_batch =\n",
        "    epochs = 10\n",
        "    n_tokens=\n",
        "    learning_rate =\n",
        "    model_name =\n",
        "    tokenizer =\n",
        "    device ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T0asPslpkSh"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset contains around 7000 Persian sentences and their corresponding polarity, and have been manually classified into 5 categories (i.e. Angry)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xpsgvYumvNa"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2JmHJ2wpoaX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = \"/content/softprompt_dataset.csv\"\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmviyjCrz6mi"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PITqCGDx0McE"
      },
      "outputs": [],
      "source": [
        "%pip install -U clean-text[gpl]\n",
        "%pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Nlfm0qE_1P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from cleantext import clean\n",
        "from hazm import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4CFqPaV0Pqp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    # regular cleaning\n",
        "    text = clean(text,\n",
        "        fix_unicode=True,\n",
        "        to_ascii=False,\n",
        "        lower=True,\n",
        "        no_line_breaks=True,\n",
        "        no_urls=True,\n",
        "        no_emails=True,\n",
        "        no_phone_numbers=True,\n",
        "        no_numbers=False,\n",
        "        no_digits=False,\n",
        "        no_currency_symbols=True,\n",
        "        no_punct=False,\n",
        "        replace_with_url=\"\",\n",
        "        replace_with_email=\"\",\n",
        "        replace_with_phone_number=\"\",\n",
        "        replace_with_number=\"\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"\",\n",
        "    )\n",
        "\n",
        "    text = cleanhtml(text)\n",
        "\n",
        "    # normalizing\n",
        "    #normalizer = hazm.Normalizer()\n",
        "    #text = normalizer.normalize(text)\n",
        "\n",
        "    wierd_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2069\"\n",
        "        u\"\\u2066\"\n",
        "        u\"\\u2068\"\n",
        "        u\"\\u2067\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = wierd_pattern.sub(r'', text)\n",
        "\n",
        "    # removing extra spaces, hashtags\n",
        "    text = re.sub(\"#\", \"\", text)\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_5ZyotA0cxw"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "def parallel_apply_with_progress(df, func, n_workers=4):\n",
        "    with ThreadPoolExecutor(max_workers=n_workers) as executor, tqdm(total=len(df)) as pbar:\n",
        "        def update(*args):\n",
        "            pbar.update()\n",
        "\n",
        "        results = []\n",
        "        for result in executor.map(func, df['text']):\n",
        "            results.append(result)\n",
        "            update()\n",
        "\n",
        "        df['text'] = pd.Series(results)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APUjLG3E0qxK"
      },
      "outputs": [],
      "source": [
        "df = parallel_apply_with_progress(df, cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2tZk2fBSwJL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
        "                                                  df.label.values,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=df.label.values)\n",
        "\n",
        "train_df = df.loc[X_train]\n",
        "validation_df = df.loc[X_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GTgl8-RV926"
      },
      "outputs": [],
      "source": [
        "possible_labels = df.label.unique()\n",
        "\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index\n",
        "label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgTnO4ctWKqo"
      },
      "outputs": [],
      "source": [
        "train_df['label'] = train_df.label.replace(label_dict)\n",
        "validation_df['label'] = validation_df.label.replace(label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIWHJ9XMiOr"
      },
      "source": [
        "### Create Dataset Class\n",
        "In this step we will getting our dataset ready for training.\n",
        "\n",
        "In this part we will define BERT-based dataset class for text classification, with configuration parameters. It preprocesses text data and tokenizes it using the BERT tokenizer.\n",
        "\n",
        "\n",
        "Complete the preprocessing step in the __getitem__ method by adding padding tokens to 'input_ids' and 'attention_mask',\n",
        "The count of this pad tokens is the same as `n_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyS6DyeZ75bZ"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.text = df['text'].values\n",
        "        self.labels = df['label'].values\n",
        "        self.all_labels = [0, 1, 2, 3, 4]\n",
        "        self.max_len = CONFIG.max_len\n",
        "        self.tokenizer = CONFIG.tokenizer\n",
        "        self.n_tokens=CONFIG.n_tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        text = ' '.join(text.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ######### Your code begins #########\n",
        "\n",
        "        ######### Your code ends ###########\n",
        "        # Get the ground-truth class label for the current sample\n",
        "        labels = #TODO\n",
        "        # Create a one-hot dictionary for the current label\n",
        "        label_dict = #TODO\n",
        "        # Convert the one-hot dictionary to a tensor\n",
        "        labels_tensor = #TODO\n",
        "        return {\n",
        "            'ids':  #TODO # Token IDs including padding and special tokens\n",
        "            'mask':  #TODO # Attention mask indicating real tokens vs padding\n",
        "            'label':  #TODO # One-hot encoded label tensor for multi-class classification\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf8AYo8cFiud"
      },
      "outputs": [],
      "source": [
        "train_dataset = BERTDataset(train_df)\n",
        "validation_dataset = BERTDataset(validation_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSWkTORNcxvx"
      },
      "source": [
        "## Define Prompt Embedding Layer\n",
        "In this part we will define our prompt layer in `PROMPTEmbedding` module.\n",
        "\n",
        "\n",
        "<font color='#AA1A73'><b>You have to complete</b></font> `initialize_embedding`,  `forward` <font color='#AA1A73'><b>functions.</b></font>\n",
        "\n",
        "In `initialize_embedding` function initialize the learned embeddings based on whether they should be initialized from the vocabulary or randomly within the specified range.\n",
        "\n",
        "In `forward` function, modify the input_embedding to extract the relevant part based on n_tokens.\n",
        "\n",
        "Repeat the learned_embedding to match the size of input_embedding.\n",
        "\n",
        "Concatenate the learned_embedding and input_embedding properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tqAU4Ubj2t8"
      },
      "outputs": [],
      "source": [
        "class PROMPTEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_layer: nn.Embedding,\n",
        "                 n_tokens: int = 20,\n",
        "                 random_range: float = 0.5,\n",
        "                 initialize_from_vocab: bool = True):\n",
        "\n",
        "        super(PROMPTEmbedding, self).__init__()\n",
        "        self.emb_layer = emb_layer\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(emb_layer,\n",
        "                                                                                   n_tokens,\n",
        "                                                                                   random_range,\n",
        "                                                                                   initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             emb_layer: nn.Embedding,\n",
        "                             n_tokens: int = 20,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "\n",
        "        if initialize_from_vocab:\n",
        "            # Initialize embeddings from the vocabulary\n",
        "            vocab_emb = #TODO\n",
        "            return vocab_emb\n",
        "        else:\n",
        "            # Initialize embeddings randomly within the specified range\n",
        "            random_emb = #TODO\n",
        "            return random_emb\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        ######### Your code begins #########\n",
        "\n",
        "        ######### Your code ends ###########\n",
        "        return concat_embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhES_23bDfXZ"
      },
      "source": [
        "## Replace model's embedding layer with our layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AMhjuooOQKA"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Define your BERT model\n",
        "# Load a pretrained BERT model for classification with 5 output labels\n",
        "model = #TODO\n",
        "\n",
        "# Get the word embedding from the BERT model\n",
        "# Extract the original word embedding layer from the BERT model\n",
        "bert_embedding_layer = #TODO\n",
        "\n",
        "# Create an instance of PROMPTEmbedding to replace it\n",
        "prompt_embedding_layer = #TODO\n",
        "\n",
        "# Set the embedding of the BERT model to the new PROMPTEmbedding instance\n",
        "#TODO\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wlsc2meajrn"
      },
      "source": [
        "## Freezing Model Parameters\n",
        "In this part we will freeze entire model except `learned_embedding`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CCq8Z1lajGC"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ud-O_Rrptq"
      },
      "source": [
        "## Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IckuDmDWRye"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Using AdamW with the configs you have already set.\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17S9KFKM1jgP"
      },
      "source": [
        "## Training & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZsfsyKAY2yu"
      },
      "source": [
        "### Define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVRa2SLDWUM9"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Sj5pT6ZDbz"
      },
      "source": [
        "### Define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f-OVsQ5_War"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = np.argmax(labels, axis=1).flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bqs9Hxi9yjz"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Evaluate the model on the validation set and return average loss, predictions, and true labels\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjO1TwDSZMWH"
      },
      "source": [
        "### Define trainng loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will implement the training loop for a model using the `tqdm` library to visualize progress. The function `train()` manages the training and evaluation of the model for a set number of epochs. It displays the training loss during each epoch and reports the validation loss and F1 score after each epoch without disrupting the progress bar display."
      ],
      "metadata": {
        "id": "OjIZMwyJCUii"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjXZ7oB_1jGv"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Train the model using training data and evaluate on validation data each epoch\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaTI1yeyZW1i"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training function is defined, you can call it to begin training your model. The following line initializes the training process by passing the model, optimizer, training data loader, and validation data loader to the train() function. Add this line of code below to execute training."
      ],
      "metadata": {
        "id": "c6lz_Yi0C7dq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--yqi1tp1jCv"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Start training the model with the specified optimizer, training data, and validation data\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_3FYFfIUKeE"
      },
      "source": [
        "## Using OpenDelta library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp3_9J0wUBxb"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/thunlp/OpenDelta.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOwAdIRUIKh"
      },
      "source": [
        "Use `OpenDelta` library to do the same thing. [link](https://opendelta.readthedocs.io/en/latest/modules/deltas.html)\n",
        "\n",
        "For hyperparameters, test with `N_SOFT_PROMPT_TOKENS=10` and `N_SOFT_PROMPT_TOKENS=20` and report them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenDelta library append soft tokens directly to the prompts so we do not need to add them by ourselves, so we need to initialize our dataset another time them without them."
      ],
      "metadata": {
        "id": "-cXQg2xfs_8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Define a custom Dataset class for handling BERT tokenization and multi-class labels\n",
        "class NewBERTDataset(Dataset):\n",
        "\n",
        "######### Your code ends ###########"
      ],
      "metadata": {
        "id": "w8kf0Pa-tgY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NewBERTDataset(train_df)\n",
        "validation_dataset = NewBERTDataset(validation_df)"
      ],
      "metadata": {
        "id": "ra7R8tERt6wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch,\n",
        "                              num_workers=2, shuffle=True, pin_memory=True)\n",
        "\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=CONFIG.valid_batch,\n",
        "                              num_workers=2, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "_KOhhvpVuEde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results in both cases show competitive performance but when `N_SOFT_PROMPT_TOKENS=10`, we have slightly better performance in terms of F1-score. We can continue this experiment with larger values for `soft_token_num` to see if performance improves or not:"
      ],
      "metadata": {
        "id": "y88vyIYZOWj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will load a pre-trained transformer model and apply soft prompt tuning using `SoftPromptModel` from the `opendelta` library. This approach prepends a set number of learnable prompt tokens to the model's input without updating the full model weights. After freezing the original model and initializing the optimizer, the training process begins using the custom `train()` function."
      ],
      "metadata": {
        "id": "zruYXibFD977"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiNnIlRlUFyS"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# CASE 1: N_SOFT_PROMPT_TOKENS = 10\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whrxYBhiIeYv"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# CASE 2: N_SOFT_PROMPT_TOKENS=20\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning"
      ],
      "metadata": {
        "id": "IbvnOeynyuki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning is the mental process of drawing conclusions, making decisions, or solving problems by thinking through information step by step. In the context of humans, it's how we make sense of things. In the context of AI and language models, it's how the model simulates a logical thought process to arrive at an answer."
      ],
      "metadata": {
        "id": "JQKGnL2cz3jW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxUrkg0u8r0W"
      },
      "source": [
        "## Chain-of-Thought (CoT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r26yOcE98r0X"
      },
      "source": [
        "LLMs have demonstrated good reasoning abilities. Furthermore, their capabilities can be further improved by incorporating reasoning techniques. One of the most notable developments in this area is the [Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903), which was introduced by Google. This approach has shown promising results in improving the reasoning capabilities of language models across a variety of tasks. Can you explain what CoT is and how it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E94KmdaIzdNj"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC13Hsv68r0X"
      },
      "source": [
        "In this section, you should use the CoT technique. firstly you need to load the [Phi-2 model](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/). This model has been introduced by Microsoft as a small LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ1v6FHF8r0Y"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Initialize the model and tokenizer, and implement the generate_output function to generate responses based on input questions\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ZVZVgi8r0Y"
      },
      "source": [
        "Use Phi-2 to answer the questions below with and without CoT. Compare results and explain their difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rJQJ3VY8r0Z"
      },
      "outputs": [],
      "source": [
        "questions = [\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "\"Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\",\n",
        "\"John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\",\n",
        "\"There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\",\n",
        "\"Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\"\n",
        "]\n",
        "\n",
        "## Correct Answers for each question:\n",
        "    # 1: $10\n",
        "    # 2: 400 ml\n",
        "    # 3: 72 hours\n",
        "    # 4: 91 chairs\n",
        "    # 5: 75 words\n",
        "\n",
        "######### Your code begins #########\n",
        "\n",
        "# Prompt the model with each question (without Chain-of-Thought) to generate direct answers\n",
        "\"\"\"Step 1: Prompting without CoT\"\"\"\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzpx2HFgk3xX"
      },
      "source": [
        "In this part, because with the determined max length we couldn't answer the last question, in the next part, we increased max_length and again did the prompting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOyGI7TajYPP"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Use the Chain-of-Thought (CoT) prompting technique to generate step-by-step answers for each question, limiting the output length to 300 tokens\n",
        "\"\"\"Step 2: Prompting with CoT - Max length: 300\"\"\"\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oc7tfq5klFk"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Use the Chain-of-Thought (CoT) prompting technique to generate step-by-step answers for each question, limiting the output length to 350 tokens\n",
        "\"\"\"Step 2: Prompting with CoT - Max length = 350\"\"\"\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sg-W9Wz8r0Z"
      },
      "source": [
        "**Results without CoT:** Among five questions we had in our propmts, the model just answered two of them correctly while for one of them (Bert example), it followed wrong steps but it concluded to correct answer.\n",
        "\n",
        "**Results with Cot:** Cot did an incredible job and model answered all of the questions correctly. In addition, all of the reasoning step were correct and model obtained answers through correct reasoning steps. But this method has a problem, it makes llm output longer than what we had in the previous step so we will need a longer `max_length` for llm output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pknDd4gH8r0Z"
      },
      "source": [
        "## Other Methods for Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e89r-dJ8r0Z"
      },
      "source": [
        "There are many other approaches to utilize the reasoning abilities of LLMs. Describe the [Tree-of-Thought (ToT)](https://arxiv.org/abs/2305.10601) and [Self-Consistency](https://arxiv.org/abs/2203.11171) within these approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMtIiwoy8r0a"
      },
      "source": [
        " **Tree of Thoughts (ToT)**:\n",
        "   - ToT is a novel approach that enhances language model (LM) inference by allowing deliberate problem solving through interconnected reasoning steps.\n",
        "   - Key features of ToT:\n",
        "     - **Coherent Units of Text (Thoughts)**: ToT maintains a tree structure where each node represents a coherent sequence of language (a \"thought\"). These thoughts serve as intermediate steps toward solving a problem.\n",
        "     - **Self-Evaluation and Decision Making**: LMs using ToT can self-evaluate their progress by considering multiple reasoning paths. They deliberate on choices and decide the next course of action based on intermediate thoughts.\n",
        "     - **Global Choices and Backtracking**: ToT enables LMs to look ahead or backtrack when necessary, allowing for global decisions that impact the overall problem-solving process.\n",
        "\n",
        "**Self-Consistency**:\n",
        "\n",
        "  Self-consistency is an advanced prompting technique that builds on COT prompting. The aim here is to improve the naive greedy decoding using COT prompting by sampling multiple diverse reasoning paths and selecting the most consistent answers. By utilizing a majority voting system, the AI model can arrive at more accurate and reliable answers.\n",
        "\n",
        "\n",
        "  To implement self-consistency, prompt engineers typically follow these steps:\n",
        "\n",
        "- **Identify the problem:** Define the problem or question for which you require LLM's assistance. Make sure it is clear and specific.\n",
        "- **Create multiple prompts:** Develop various prompts that approach the problem from different angles or perspectives. Each prompt should provide a unique reasoning path for the AI to follow.\n",
        "- **Generate responses:** Submit the prompts to LLM and obtain the responses generated by the model.\n",
        "- **Evaluate consistency:** Analyze the generated responses to determine their coherence, relevance, and consistency. This step may involve comparing the responses to each other, looking for common themes or patterns, and checking for internal logical consistency.\n",
        "- **Select the best response:** Based on the evaluation, choose the most consistent and accurate response as the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j3GS92m8r0a"
      },
      "source": [
        "Now, implement Self-Consistency to answer the questions of the previous section.\n",
        "Analyze the results obtained from Steps 1 and 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WPY14Hut3wm"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzIqzHWq43hR"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Apply multiple Chain-of-Thought (CoT) prompts with self-consistency to generate answers and compare the results across different prompt variations\n",
        "# **** Question: diff prompts? we should recognize it by ourslves or not\n",
        "\"\"\"Step 3: Prompting with CoT and Self-Consistency - Max length = 350\"\"\"\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S72rMyRB8r0a"
      },
      "source": [
        "Consider LLMs' features and propose a new approach based on them to enhance LLMs' reasoning abilities. Why do you believe this approach could enhance LLMs' reasoning abilities?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA5u6MoiSsRC"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7xpsgvYumvNa",
        "qmviyjCrz6mi"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}