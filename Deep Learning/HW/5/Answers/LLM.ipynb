{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://logoyab.com/wp-content/uploads/2024/08/IUST-University-Logo-1030x1030.png\" alt=\"Logo\" width=\"200\">\n",
        "    <p><b>HW5 @ Deep Learning Course, Dr. Mohammadi</b></p>\n",
        "    <p><b>ِDesinged by Nafiseh Ahmadi</b></p>\n",
        "</div>\n",
        "\n",
        "--------\n"
      ],
      "metadata": {
        "id": "2YnI5jQP6i3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Full Name:* Mohammad Haghighat\n",
        "\n",
        "*Student Number:* 403722042\n",
        "\n",
        "\n",
        "------\n"
      ],
      "metadata": {
        "id": "XXmbE_pE6hyr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfrgatGB3Aac"
      },
      "source": [
        "# What are Soft prompts?\n",
        "Soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset; the downside is that they aren’t human readable because you aren’t matching these “virtual tokens” to the embeddings of a real word.\n",
        "<br>\n",
        "<div>\n",
        "<img src=\"https://www.researchgate.net/publication/366062946/figure/fig1/AS:11431281105340756@1670383256990/The-comparison-between-the-previous-T5-prompt-tuning-method-part-a-and-the-introduced.jpg\">\n",
        "</div>\n",
        "\n",
        "Read More:\n",
        "<br>[Youtube : PEFT and Soft Prompt](https://www.youtube.com/watch?v=8uy_WII76L0)\n",
        "<br>[Paper: The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
        "https://arxiv.org/pdf/2101.00190.pdf\n",
        "<br>[Paper: Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLaYIRN4X7cC"
      },
      "source": [
        "# Part 1\n",
        "Before diving into the practical applications, let's first ensure your foundational knowledge is solid. Please answer the following questions.\n",
        "\n",
        "\n",
        "**A) Compare and contrast model tuning and prompt tuning in terms of their effectiveness for specific downstream tasks.**\n",
        "\n",
        "**B) Explore the challenges associated with interpreting soft prompts in the continuous embedding space and propose potential solutions.**\n",
        "\n",
        "**C) What is the effect of initializing prompts randomly versus initializing them from the vocabulary, and how does this impact the performance of prompt tuning?**\n",
        "\n",
        "**D) How is the optimization process in the prefix tuning(<br>[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)) and Why did they use this technique?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9CqqGdLMpvO"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nH1uEWfwzRqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W0zxen79zI-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>A :</b></font>\n",
        "<p dir='rtl'>مقایسه تنظیم مدل و تنظیم اعلان از نظر اثربخشی برای وظایف خاص پایین‌دستی.\n",
        "</p><p dir='rtl'>\n",
        "تنظیم کامل مدل (Model Tuning)، که شامل به‌روزرسانی تمامی یا بخش قابل توجهی از پارامترهای یک مدل زبانی بزرگ است، عموماً اثربخشی بالاتری برای طیف وسیعی از وظایف پیچیده پایین‌دستی ارائه می‌دهد، زیرا دانش تخصصی مربوط به وظیفه را عمیقاً در مدل نهادینه می‌کند. در مقابل، تنظیم اعلان (Prompt Tuning)، که تنها مجموعه کوچکی از پارامترهای مختص اعلان (اعلان‌های نرم) را که به ابتدای ورودی اضافه می‌شوند تنظیم می‌کند، می‌تواند برای بسیاری از وظایف، به ویژه در شرایط محدودیت منابع محاسباتی یا برای وظایفی که نیاز به انطباق سریع دارند، به طرز شگفت‌آوری مؤثر باشد. در حالی که تنظیم کامل مدل ممکن است به اوج عملکرد دست یابد، تنظیم اعلان جایگزینی با پارامترهای بهینه‌تر ارائه می‌دهد و در سناریوهایی که تفاوت‌های ظریف مربوط به وظیفه را می‌توان با شرطی‌سازی مناسب مدل پایه منجمد شده دریافت، برتری می‌یابد؛ هرچند ممکن است در وظایفی که نیازمند تزریق دانش گسترده یا تغییرات بنیادی در رفتار مدل هستند، عملکرد ضعیف‌تری داشته باشد.\n",
        "</p>"
      ],
      "metadata": {
        "id": "nCEZTUl18cy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>B :</b></font>\n",
        "<p dir='rtl'>\n",
        "تفسیر اعلان‌های نرم در فضای پیوسته برداری چالش‌برانگیز است، زیرا این بردارهای یادگرفته‌شده، برخلاف کلمات گسسته، معادل‌های مستقیم و قابل فهم برای انسان ندارند. برخلاف اعلان‌های سخت که از متن قابل تفسیر تشکیل شده‌اند، اعلان‌های نرم برای عملکرد بهینه شده‌اند و اغلب منجر به مقادیر برداری می‌شوند که به طور واضح به واژگان یا مفاهیم موجود نگاشت نمی‌شوند و عملکرد آن‌ها را مبهم می‌سازند. راه‌حل‌های بالقوه شامل کاهش ابعاد و تکنیک‌های بصری‌سازی برای شناسایی خوشه‌ها یا الگوها در فضای برداری، روش‌های کاوش (probing) که یک مدل جداگانه را برای پیش‌بینی ویژگی‌ها بر اساس بردارهای اعلان نرم آموزش می‌دهند، و تولید اعلان استهلاکی (amortized prompt generation) است که در آن اعلان نرم توسط شبکه دیگری که با ورودی‌های قابل تفسیر شرطی شده است تولید می‌شود، و بدین ترتیب به طور غیرمستقیم بینشی در مورد اطلاعاتی که اعلان ممکن است رمزگذاری کند، ارائه می‌دهد.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Wn9lBBUY8i7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>C :</b></font>\n",
        "<p dir='rtl'>مقداردهی اولیه تصادفی اعلان‌ها در مقابل مقداردهی اولیه آن‌ها از واژگان به طور قابل توجهی بر نقطه شروع و مسیر فرآیند بهینه‌سازی تنظیم اعلان تأثیر می‌گذارد. مقداردهی اولیه تصادفی، ضمن ارائه فضای اکتشافی وسیع‌تر، می‌تواند منجر به همگرایی کندتر یا گیر افتادن در کمینه‌های محلی نامطلوب شود، زیرا مدل باید از ابتدا بردارهای اعلان مفید را یاد بگیرد. مقداردهی اولیه با بردارهای کلمات واژگان، حتی اگر این کلمات برای وظیفه بهینه نباشند، نقطه شروع آگاهانه‌تری را فراهم می‌کند که اغلب به ناحیه مفیدتری در فضای برداری نزدیک‌تر است. این امر می‌تواند منجر به همگرایی سریع‌تر و گاهی اوقات عملکرد نهایی بهتر شود، زیرا اعلان اولیه از قبل حاوی مقداری معنای معنایی است که مدل می‌تواند بر اساس آن بنا کند و به طور بالقوه بهینه‌سازی را به سمت بردارهای اعلان قابل تفسیرتر یا مؤثرتر هدایت کند.</p>"
      ],
      "metadata": {
        "id": "NP__pgyy8iyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#FA5170'><b>D :</b></font>\n",
        "<p dir='rtl'>در پیشوند-تیونینگ، فرآیند بهینه‌سازی شامل ثابت نگه داشتن پارامترهای مدل زبانی بزرگ و تنها به‌روزرسانی پارامترهای \"پیشوند\" است که دنباله‌ای از بردارهای پیوسته و مختص وظیفه می‌باشد و در هر لایه از ترانسفورمر به ابتدای ورودی اضافه می‌شود. این تکنیک به این دلیل مورد استفاده قرار گرفت که روشی بهینه از نظر تعداد پارامترها برای انطباق مدل‌های بزرگ از پیش آموزش‌دیده با وظایف مختلف تولید زبان طبیعی پایین‌دستی ارائه می‌دهد، بدون آنکه نیازی به تنظیم دقیق کل مدل عظیم برای هر وظیفه خاص باشد. با بهینه‌سازی تنها پیشوند کوچک، پیشوند-تیونینگ به طور قابل توجهی هزینه‌های محاسباتی و ذخیره‌سازی به ازای هر وظیفه را کاهش می‌دهد و در عین حال به عملکردی قابل مقایسه با تنظیم دقیق کامل دست می‌یابد، که آن را برای سفارشی‌سازی مدل‌های بزرگ بسیار مقیاس‌پذیر و عملی می‌سازد.</p>"
      ],
      "metadata": {
        "id": "_0B4ZZ6Z8isS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQMgi75DV-bo"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3gjmZROpLP5"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZG_yY6ep9C7S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_UcfBmLpRiJ"
      },
      "source": [
        "## Model Selection & Constants\n",
        "We will use `bert-fa-base-uncased` as our base model from Hugging Face ([HF_Link](https://huggingface.co/HooshvareLab/bert-fa-base-uncased)). For our tuning, we intend to utilize 20 soft prompt tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "del7X7TWnAvN",
        "outputId": "f3479acf-ef0d-4d13-c4df-366053c11154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m131.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed transformers-4.52.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "ff9d0d13d0a1460390a7cabbbc1859a8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mV2aP8bwV-z5"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "    seed = 42\n",
        "    max_len = 128\n",
        "    train_batch = 16\n",
        "    valid_batch = 32\n",
        "    epochs = 10\n",
        "    n_tokens=20\n",
        "    learning_rate = 0.01\n",
        "    model_name = 'HooshvareLab/bert-fa-base-uncased'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T0asPslpkSh"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset contains around 7000 Persian sentences and their corresponding polarity, and have been manually classified into 5 categories (i.e. Angry)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xpsgvYumvNa"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b2JmHJ2wpoaX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = \"/content/softprompt_dataset.csv\"\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmviyjCrz6mi"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PITqCGDx0McE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749d26bf-ed9e-4ab1-929b-9bd7089d97c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: clean-text[gpl] in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from clean-text[gpl]) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from clean-text[gpl]) (6.3.1)\n",
            "Requirement already satisfied: unidecode<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from clean-text[gpl]) (1.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U clean-text[gpl]\n",
        "#%pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U0Nlfm0qE_1P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from cleantext import clean\n",
        "#from hazm import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k4CFqPaV0Pqp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    # regular cleaning\n",
        "    text = clean(text,\n",
        "        fix_unicode=True,\n",
        "        to_ascii=False,\n",
        "        lower=True,\n",
        "        no_line_breaks=True,\n",
        "        no_urls=True,\n",
        "        no_emails=True,\n",
        "        no_phone_numbers=True,\n",
        "        no_numbers=False,\n",
        "        no_digits=False,\n",
        "        no_currency_symbols=True,\n",
        "        no_punct=False,\n",
        "        replace_with_url=\"\",\n",
        "        replace_with_email=\"\",\n",
        "        replace_with_phone_number=\"\",\n",
        "        replace_with_number=\"\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"\",\n",
        "    )\n",
        "\n",
        "    text = cleanhtml(text)\n",
        "\n",
        "    # normalizing\n",
        "    #normalizer = hazm.Normalizer()\n",
        "    #text = normalizer.normalize(text)\n",
        "\n",
        "    wierd_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2069\"\n",
        "        u\"\\u2066\"\n",
        "        u\"\\u2068\"\n",
        "        u\"\\u2067\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = wierd_pattern.sub(r'', text)\n",
        "\n",
        "    # removing extra spaces, hashtags\n",
        "    text = re.sub(\"#\", \"\", text)\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z_5ZyotA0cxw"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "def parallel_apply_with_progress(df, func, n_workers=4):\n",
        "    with ThreadPoolExecutor(max_workers=n_workers) as executor, tqdm(total=len(df)) as pbar:\n",
        "        def update(*args):\n",
        "            pbar.update()\n",
        "\n",
        "        results = []\n",
        "        for result in executor.map(func, df['text']):\n",
        "            results.append(result)\n",
        "            update()\n",
        "\n",
        "        df['text'] = pd.Series(results)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "APUjLG3E0qxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387e8228-2be5-4e4d-f1e9-c590c3df08fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7023/7023 [00:01<00:00, 3636.63it/s]\n"
          ]
        }
      ],
      "source": [
        "df = parallel_apply_with_progress(df, cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_2tZk2fBSwJL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
        "                                                  df.label.values,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=df.label.values)\n",
        "\n",
        "train_df = df.loc[X_train]\n",
        "validation_df = df.loc[X_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3GTgl8-RV926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ec35ef-ef2d-4bcc-ced5-6ef599efe723"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{np.int64(0): 0,\n",
              " np.int64(1): 1,\n",
              " np.int64(2): 2,\n",
              " np.int64(-1): 3,\n",
              " np.int64(-2): 4}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "possible_labels = df.label.unique()\n",
        "\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index\n",
        "label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IgTnO4ctWKqo"
      },
      "outputs": [],
      "source": [
        "train_df['label'] = train_df.label.replace(label_dict)\n",
        "validation_df['label'] = validation_df.label.replace(label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIWHJ9XMiOr"
      },
      "source": [
        "### Create Dataset Class\n",
        "In this step we will getting our dataset ready for training.\n",
        "\n",
        "In this part we will define BERT-based dataset class for text classification, with configuration parameters. It preprocesses text data and tokenizes it using the BERT tokenizer.\n",
        "\n",
        "\n",
        "Complete the preprocessing step in the __getitem__ method by adding padding tokens to 'input_ids' and 'attention_mask',\n",
        "The count of this pad tokens is the same as `n_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CyS6DyeZ75bZ"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.text = df['text'].values\n",
        "        self.labels = df['label'].values\n",
        "        self.all_labels = [0, 1, 2, 3, 4]\n",
        "        self.max_len = CONFIG.max_len\n",
        "        self.tokenizer = CONFIG.tokenizer\n",
        "        self.n_tokens=CONFIG.n_tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        text = ' '.join(text.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ######### Your code begins #########\n",
        "        inputs['input_ids'] = torch.tensor(inputs['input_ids'] + [self.tokenizer.pad_token_id]*self.n_tokens, dtype=torch.long)\n",
        "        inputs['attention_mask'] = torch.tensor(inputs['attention_mask'] + [0]*self.n_tokens, dtype=torch.long)\n",
        "        ######### Your code ends ###########\n",
        "        # Get the ground-truth class label for the current sample\n",
        "        labels = self.labels[index]\n",
        "        # Create a one-hot dictionary for the current label\n",
        "        label_dict = {label: (label == labels) for label in self.all_labels}\n",
        "        # Convert the one-hot dictionary to a tensor\n",
        "        labels_tensor = torch.tensor([float(label_dict[label]) for label in self.all_labels])\n",
        "        return {\n",
        "            'ids':  inputs['input_ids'], # Token IDs including padding and special tokens\n",
        "            'mask':  inputs['attention_mask'], # Attention mask indicating real tokens vs padding\n",
        "            'label':  labels_tensor # One-hot encoded label tensor for multi-class classification\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qf8AYo8cFiud"
      },
      "outputs": [],
      "source": [
        "train_dataset = BERTDataset(train_df)\n",
        "validation_dataset = BERTDataset(validation_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSWkTORNcxvx"
      },
      "source": [
        "## Define Prompt Embedding Layer\n",
        "In this part we will define our prompt layer in `PROMPTEmbedding` module.\n",
        "\n",
        "\n",
        "<font color='#AA1A73'><b>You have to complete</b></font> `initialize_embedding`,  `forward` <font color='#AA1A73'><b>functions.</b></font>\n",
        "\n",
        "In `initialize_embedding` function initialize the learned embeddings based on whether they should be initialized from the vocabulary or randomly within the specified range.\n",
        "\n",
        "In `forward` function, modify the input_embedding to extract the relevant part based on n_tokens.\n",
        "\n",
        "Repeat the learned_embedding to match the size of input_embedding.\n",
        "\n",
        "Concatenate the learned_embedding and input_embedding properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6tqAU4Ubj2t8"
      },
      "outputs": [],
      "source": [
        "class PROMPTEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_layer: nn.Embedding,\n",
        "                 n_tokens: int = 20,\n",
        "                 random_range: float = 0.5,\n",
        "                 initialize_from_vocab: bool = True):\n",
        "\n",
        "        super(PROMPTEmbedding, self).__init__()\n",
        "        self.emb_layer = emb_layer\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(emb_layer,\n",
        "                                                                                   n_tokens,\n",
        "                                                                                   random_range,\n",
        "                                                                                   initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             emb_layer: nn.Embedding,\n",
        "                             n_tokens: int = 20,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "\n",
        "        if initialize_from_vocab:\n",
        "            # Initialize embeddings from the vocabulary\n",
        "            vocab_emb = self.emb_layer.weight[:n_tokens].clone().detach()\n",
        "            return vocab_emb\n",
        "        else:\n",
        "            # Initialize embeddings randomly within the specified range\n",
        "            random_emb = torch.FloatTensor(n_tokens, emb_layer.weight.size(1)).uniform_(-random_range, random_range)\n",
        "            return random_emb\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        ######### Your code begins #########\n",
        "        input_embedding = self.emb_layer(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        joined_embedding = torch.cat([learned_embedding, input_embedding], dim=1)\n",
        "        ######### Your code ends ###########\n",
        "        return concat_embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhES_23bDfXZ"
      },
      "source": [
        "## Replace model's embedding layer with our layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-AMhjuooOQKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3096d4a-a2cf-4796-e5d9-4fead05f253a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Define your BERT model\n",
        "# Load a pretrained BERT model for classification with 5 output labels\n",
        "model = AutoModelForSequenceClassification.from_pretrained(CONFIG.model_name, num_labels=5, output_attentions = False,\n",
        "                                                           output_hidden_states = False).to(CONFIG.device)\n",
        "\n",
        "# Get the word embedding from the BERT model\n",
        "# Extract the original word embedding layer from the BERT model\n",
        "bert_embedding_layer = model.bert.embeddings.word_embeddings\n",
        "\n",
        "\n",
        "# Set the embedding of the BERT model to the new PROMPTEmbedding instance\n",
        "prompt_embedding_layer = PROMPTEmbedding(\n",
        "    emb_layer=bert_embedding_layer,\n",
        "    n_tokens=CONFIG.n_tokens,\n",
        "    random_range=0.5,\n",
        "    initialize_from_vocab=True\n",
        ")\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wlsc2meajrn"
      },
      "source": [
        "## Freezing Model Parameters\n",
        "In this part we will freeze entire model except `learned_embedding`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3CCq8Z1lajGC"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "for name, param in model.named_parameters():\n",
        "    if 'learned_embedding' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "for param in model.get_input_embeddings().parameters():\n",
        "    param.requires_grad = True\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ud-O_Rrptq"
      },
      "source": [
        "## Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1IckuDmDWRye"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Using AdamW with the configs you have already set.\n",
        "optimizer = AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17S9KFKM1jgP"
      },
      "source": [
        "## Training & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZsfsyKAY2yu"
      },
      "source": [
        "### Define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mVRa2SLDWUM9"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch,\n",
        "                              num_workers=2, shuffle=True, pin_memory=True)\n",
        "\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=CONFIG.valid_batch,\n",
        "                              num_workers=2, shuffle=True, pin_memory=True)\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Sj5pT6ZDbz"
      },
      "source": [
        "### Define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3f-OVsQ5_War"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = np.argmax(labels, axis=1).flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-bqs9Hxi9yjz"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "def evaluate(val_dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "\n",
        "\n",
        "        inputs = {'input_ids':      batch['ids'].to(CONFIG.device),\n",
        "                  'attention_mask': batch['mask'].to(CONFIG.device),\n",
        "                  'labels':         batch['label'].to(CONFIG.device),\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[\"loss\"]\n",
        "        logits = outputs[\"logits\"]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "\n",
        "    loss_val_avg = loss_val_total/len(val_dataloader)\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "\n",
        "    return loss_val_avg, predictions, true_vals\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjO1TwDSZMWH"
      },
      "source": [
        "### Define trainng loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will implement the training loop for a model using the `tqdm` library to visualize progress. The function `train()` manages the training and evaluation of the model for a set number of epochs. It displays the training loss during each epoch and reports the validation loss and F1 score after each epoch without disrupting the progress bar display."
      ],
      "metadata": {
        "id": "OjIZMwyJCUii"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xjXZ7oB_1jGv"
      },
      "outputs": [],
      "source": [
        "######### Your code begins #########\n",
        "def train(model, optimizer, train_dataloader, val_dataloader):\n",
        "\n",
        "    epochs = CONFIG.epochs\n",
        "\n",
        "    for epoch in tqdm(range(1, epochs+1)):\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      loss_train_total = 0\n",
        "\n",
        "      progress_bar = tqdm(train_loader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=True)\n",
        "\n",
        "      for batch in progress_bar:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = {'input_ids':      batch['ids'].to(CONFIG.device),\n",
        "                  'attention_mask': batch['mask'].to(CONFIG.device),\n",
        "                  'labels':         batch['label'].to(CONFIG.device),\n",
        "                }\n",
        "\n",
        "        output = model(**inputs)\n",
        "\n",
        "        loss = output[\"loss\"]\n",
        "        loss_train_total += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "\n",
        "\n",
        "      tqdm.write(f'\\nEpoch {epoch}')\n",
        "      loss_train_avg = loss_train_total/len(train_loader)\n",
        "      tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "\n",
        "\n",
        "      val_loss, predictions, true_vals = evaluate(val_dataloader)\n",
        "      val_f1 = f1_score_func(predictions, true_vals)\n",
        "      tqdm.write(f'Validation loss: {val_loss}')\n",
        "      tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
        "\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaTI1yeyZW1i"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training function is defined, you can call it to begin training your model. The following line initializes the training process by passing the model, optimizer, training data loader, and validation data loader to the train() function. Add this line of code below to execute training."
      ],
      "metadata": {
        "id": "c6lz_Yi0C7dq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "--yqi1tp1jCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39380659-56ee-4c66-9e1f-a4b3eace04af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [01:34<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training loss: 0.41716803430734317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [01:42<15:19, 102.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.36283187568187714\n",
            "F1 Score (Weighted): 0.6282995728396591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [03:14<15:19, 102.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "Training loss: 0.3460669246675137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:22<13:30, 101.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3407060205936432\n",
            "F1 Score (Weighted): 0.6552977420312335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [04:55<13:30, 101.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "Training loss: 0.31612477934515826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [05:03<11:46, 100.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.34222824179700445\n",
            "F1 Score (Weighted): 0.6413658388415912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [06:35<11:46, 100.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "Training loss: 0.29306236341450037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:43<10:03, 100.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.341148568051202\n",
            "F1 Score (Weighted): 0.644918735248681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [08:16<10:03, 100.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "Training loss: 0.2768393882128625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [08:24<08:23, 100.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.33991838565894533\n",
            "F1 Score (Weighted): 0.6377861938023976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [09:57<08:23, 100.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6\n",
            "Training loss: 0.266058927589985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [10:04<06:42, 100.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3415884721492018\n",
            "F1 Score (Weighted): 0.632716844653574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [11:37<06:42, 100.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7\n",
            "Training loss: 0.2545024850178231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [11:45<05:01, 100.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.34301896393299103\n",
            "F1 Score (Weighted): 0.6357681406680463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [13:17<05:01, 100.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8\n",
            "Training loss: 0.24810545073658324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [13:25<03:21, 100.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3586620220116207\n",
            "F1 Score (Weighted): 0.6265702587199926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [14:58<03:21, 100.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9\n",
            "Training loss: 0.2412648937309992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [15:06<01:40, 100.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.34674721690160887\n",
            "F1 Score (Weighted): 0.630797616229409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [16:38<01:40, 100.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10\n",
            "Training loss: 0.23717531950155377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [16:46<00:00, 100.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3624980353883335\n",
            "F1 Score (Weighted): 0.6131764582477426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Start training the model with the specified optimizer, training data, and validation data\n",
        "train(model=model, optimizer=optimizer, train_dataloader=train_loader, val_dataloader=validation_loader)\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_3FYFfIUKeE"
      },
      "source": [
        "## Using OpenDelta library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOwAdIRUIKh"
      },
      "source": [
        "Use `OpenDelta` library to do the same thing. [link](https://opendelta.readthedocs.io/en/latest/modules/deltas.html)\n",
        "\n",
        "For hyperparameters, test with `N_SOFT_PROMPT_TOKENS=10` and `N_SOFT_PROMPT_TOKENS=20` and report them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenDelta library append soft tokens directly to the prompts so we do not need to add them by ourselves, so we need to initialize our dataset another time them without them."
      ],
      "metadata": {
        "id": "-cXQg2xfs_8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/thunlp/OpenDelta.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PcX4C9zfUN5r",
        "outputId": "947caeec-201a-49ec-95f6-300319dcd67a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/thunlp/OpenDelta.git\n",
            "  Cloning https://github.com/thunlp/OpenDelta.git to /tmp/pip-req-build-nfd9cem9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/thunlp/OpenDelta.git /tmp/pip-req-build-nfd9cem9\n",
            "  Resolved https://github.com/thunlp/OpenDelta.git to commit 9efab85a6eac2bc8949f71937492b43455bdf4a7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (4.52.4)\n",
            "Requirement already satisfied: datasets>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (2.14.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (0.2.0)\n",
            "Requirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (4.67.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (4.4.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (13.9.4)\n",
            "Collecting web.py (from opendelta==0.3.2)\n",
            "  Downloading web.py-0.62.tar.gz (623 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m623.2/623.2 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (3.1.44)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from opendelta==0.3.2) (1.6.1)\n",
            "Collecting delta_center_client==0.0.4 (from opendelta==0.3.2)\n",
            "  Downloading delta_center_client-0.0.4-py3-none-any.whl.metadata (801 bytes)\n",
            "Collecting bigmodelvis (from opendelta==0.3.2)\n",
            "  Downloading bigmodelvis-0.0.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting oss2==2.15.0 (from delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading oss2-2.15.0.tar.gz (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yacs>=0.1.6 (from delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: requests!=2.9.0 in /usr/local/lib/python3.11/dist-packages (from oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (2.32.3)\n",
            "Collecting crcmod>=1.7 (from oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets>=1.17.0->opendelta==0.3.2) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.17.0->opendelta==0.3.2) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->opendelta==0.3.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->opendelta==0.3.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->opendelta==0.3.2) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.10.0->opendelta==0.3.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.10.0->opendelta==0.3.2) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.10.0->opendelta==0.3.2) (0.5.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->opendelta==0.3.2) (4.0.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->opendelta==0.3.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->opendelta==0.3.2) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->opendelta==0.3.2) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->opendelta==0.3.2) (3.6.0)\n",
            "Collecting cheroot (from web.py->opendelta==0.3.2)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.17.0->opendelta==0.3.2) (1.20.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->opendelta==0.3.2) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->opendelta==0.3.2) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests!=2.9.0->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests!=2.9.0->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests!=2.9.0->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests!=2.9.0->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (2025.4.26)\n",
            "Requirement already satisfied: more-itertools>=2.6 in /usr/local/lib/python3.11/dist-packages (from cheroot->web.py->opendelta==0.3.2) (10.7.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from cheroot->web.py->opendelta==0.3.2) (4.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->opendelta==0.3.2) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.17.0->opendelta==0.3.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.17.0->opendelta==0.3.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.17.0->opendelta==0.3.2) (2025.2)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta_center_client==0.0.4->opendelta==0.3.2) (2.22)\n",
            "Downloading delta_center_client-0.0.4-py3-none-any.whl (8.5 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bigmodelvis-0.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: opendelta, oss2, web.py, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for opendelta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opendelta: filename=opendelta-0.3.2-py3-none-any.whl size=83310 sha256=97d6105126e41e055b88704f535b1ceedb06531c9e7539c5093ae8913a11ceb2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3abm32zs/wheels/93/8b/a0/95f024bcc7c29e4ae1742515c58b6558cabb3750fa73ee4de5\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.15.0-py3-none-any.whl size=103429 sha256=08e5b2248324c2b39da872e3596bfdbd98cfcc6f82e4d328474f1c981877d6d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/dc/8a/3939f8b42c9b9d37ea23ff3eb7745236cbf0e6ec2da4731abf\n",
            "  Building wheel for web.py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for web.py: filename=web_py-0.62-py3-none-any.whl size=78641 sha256=6c179fd448987928a3e25279ec022fc35133a07ae700af82149f4d3fe5f95512\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/56/6e/0e399681158faaa4d3b15c5d095b0047357e1c727e6c3fd004\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535315 sha256=267c7201869c4961d4430944ff986564db251069a0408415799e12dbad40d38f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/9a/95/60f111d2a488c5f7f7ed2a96ce407ea57ec7393ddfdec8c956\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-linux_x86_64.whl size=31658 sha256=8506556a68dfcb915a8329cf52d4b0995743193ad52053a5d1f1832b8fcab8fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
            "Successfully built opendelta oss2 web.py aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: crcmod, yacs, pycryptodome, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cheroot, web.py, nvidia-cusolver-cu12, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, bigmodelvis, delta_center_client, opendelta\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 bigmodelvis-0.0.1 cheroot-10.0.1 crcmod-1.7 delta_center_client-0.0.4 jmespath-0.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opendelta-0.3.2 oss2-2.15.0 pycryptodome-3.23.0 web.py-0.62 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "7442d0acc5ad40778b69ca55e9395e0f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.28.1 deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrXPjsgaUV3b",
        "outputId": "cdcfbe40-b832-4bc1-9743-4e4e62e5b443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.28.1\n",
            "  Using cached transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
            "Collecting deepspeed\n",
            "  Using cached deepspeed-0.16.9.tar.gz (1.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (4.67.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.9-py3-none-any.whl size=1644433 sha256=25e0fb329bfaee962b19abdf98b8f9addbe7f69b69934304a4a110df01e73148\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ac/79/a454936d70b601056346a28d4347456cbdf57587851eab3cf3\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tokenizers, hjson, ninja, transformers, deepspeed\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deepspeed-0.16.9 hjson-3.1.0 ninja-1.11.1.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Requirement already satisfied: transformers==4.28.1 in /usr/local/lib/python3.11/dist-packages (4.28.1)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.11/dist-packages (0.16.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.1) (4.67.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.11/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.1) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### Your code begins #########\n",
        "from opendelta import SoftPromptModel\n",
        "# Define a custom Dataset class for handling BERT tokenization and multi-class labels\n",
        "class NewBERTDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.text = df['text'].values\n",
        "        self.labels = df['label'].values\n",
        "        self.all_labels = [0, 1, 2, 3, 4]\n",
        "        self.max_len = CONFIG.max_len\n",
        "        self.tokenizer = CONFIG.tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        text = ' '.join(text.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        inputs['input_ids'] = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
        "        inputs['attention_mask'] = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
        "        labels = self.labels[index]\n",
        "        label_dict = {label: (label == labels) for label in self.all_labels}\n",
        "        labels_tensor = torch.tensor([float(label_dict[label]) for label in self.all_labels])\n",
        "\n",
        "        return {\n",
        "            'ids': inputs['input_ids'],\n",
        "            'mask': inputs['attention_mask'],\n",
        "            'label': labels_tensor\n",
        "        }\n",
        "\n",
        "######### Your code ends ###########"
      ],
      "metadata": {
        "id": "w8kf0Pa-tgY9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NewBERTDataset(train_df)\n",
        "validation_dataset = NewBERTDataset(validation_df)"
      ],
      "metadata": {
        "id": "ra7R8tERt6wH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch,\n",
        "                              num_workers=2, shuffle=True, pin_memory=True)\n",
        "\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=CONFIG.valid_batch,\n",
        "                              num_workers=2, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "_KOhhvpVuEde"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results in both cases show competitive performance but when `N_SOFT_PROMPT_TOKENS=10`, we have slightly better performance in terms of F1-score. We can continue this experiment with larger values for `soft_token_num` to see if performance improves or not:"
      ],
      "metadata": {
        "id": "y88vyIYZOWj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will load a pre-trained transformer model and apply soft prompt tuning using `SoftPromptModel` from the `opendelta` library. This approach prepends a set number of learnable prompt tokens to the model's input without updating the full model weights. After freezing the original model and initializing the optimizer, the training process begins using the custom `train()` function."
      ],
      "metadata": {
        "id": "zruYXibFD977"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LiNnIlRlUFyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a86c137-57d5-4fb4-e5cd-f65103f7c8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with N_SOFT_PROMPT_TOKENS = 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/10 [01:30<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training loss: 0.44823510761566976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [01:37<14:34, 97.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.403165472276283\n",
            "F1 Score (Weighted): 0.521064241753424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [03:09<14:34, 97.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "Training loss: 0.3905586290327623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [03:17<13:11, 98.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3843500252925988\n",
            "F1 Score (Weighted): 0.5864958008258725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 2/10 [04:51<13:11, 98.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "Training loss: 0.36010492939681293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [04:59<11:42, 100.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.38372075738328876\n",
            "F1 Score (Weighted): 0.5678663851212205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [06:35<11:42, 100.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "Training loss: 0.33751544850395326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [06:43<10:10, 101.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.37969039877255756\n",
            "F1 Score (Weighted): 0.5821068829290885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [08:19<10:10, 101.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "Training loss: 0.32061336599888013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [08:27<08:32, 102.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3827719706477541\n",
            "F1 Score (Weighted): 0.5772164968611221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [10:03<08:32, 102.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6\n",
            "Training loss: 0.3115698100331633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [10:11<06:51, 102.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.38185083866119385\n",
            "F1 Score (Weighted): 0.5817016968100674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [11:47<06:51, 102.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7\n",
            "Training loss: 0.2989573287055454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [11:55<05:09, 103.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.38572711474967725\n",
            "F1 Score (Weighted): 0.583071883599485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 7/10 [13:31<05:09, 103.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8\n",
            "Training loss: 0.28982907545598435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [13:39<03:27, 103.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3878475364410516\n",
            "F1 Score (Weighted): 0.5725454530803633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [15:15<03:27, 103.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9\n",
            "Training loss: 0.28267300634938763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [15:23<01:43, 103.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3889337263324044\n",
            "F1 Score (Weighted): 0.5574925610444074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [16:59<01:43, 103.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10\n",
            "Training loss: 0.2770131064608773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [17:07<00:00, 102.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.389098789655801\n",
            "F1 Score (Weighted): 0.5737155513729933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training with N_SOFT_PROMPT_TOKENS = 10\")\n",
        "\n",
        "# Load pretrained BERT model\n",
        "model_10 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG.model_name,\n",
        "    num_labels=5,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False)\n",
        "\n",
        "# Apply soft prompt tuning with OpenDelta\n",
        "soft_prompt_model_10 = SoftPromptModel(backbone_model=model_10, soft_token_num=10)\n",
        "\n",
        "for name, param in model_10.named_parameters():\n",
        "    if 'soft' not in name:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Move the entire model (including soft prompts) to the correct device\n",
        "model_10 = model_10.to(CONFIG.device)\n",
        "optimizer_10 = AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train(model=model, optimizer=optimizer_10, train_dataloader=train_loader, val_dataloader=validation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "whrxYBhiIeYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa642395-be70-4eea-cc6f-d98ff61f9e64"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with N_SOFT_PROMPT_TOKENS = 20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/10 [01:41<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training loss: 0.46667389332610654\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:51<16:40, 111.20s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.45453072677959094\n",
            "F1 Score (Weighted): 0.25546864130894925\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [03:36<16:40, 111.20s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Training loss: 0.4549985918450483\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [03:46<15:10, 113.76s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.4524215604319717\n",
            "F1 Score (Weighted): 0.2334127188030517\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [05:33<15:10, 113.76s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Training loss: 0.44274358929478547\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [05:44<13:27, 115.36s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.4163389061436509\n",
            "F1 Score (Weighted): 0.3386765949749608\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [07:32<13:27, 115.36s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Training loss: 0.4285011698696065\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [07:42<11:39, 116.57s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.40804751804380707\n",
            "F1 Score (Weighted): 0.3495388168448732\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [09:30<11:39, 116.57s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Training loss: 0.42170202939268103\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [09:41<09:46, 117.37s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.4003402428193526\n",
            "F1 Score (Weighted): 0.4342901602044214\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [11:29<09:46, 117.37s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Training loss: 0.41497017801764174\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [11:39<07:51, 117.83s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.42148131041815784\n",
            "F1 Score (Weighted): 0.3825717544077922\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [13:28<07:51, 117.83s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Training loss: 0.4142167800091167\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [13:39<05:54, 118.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.40455995906483044\n",
            "F1 Score (Weighted): 0.39739500457511256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [15:26<05:54, 118.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Training loss: 0.40832218169209794\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [15:37<03:56, 118.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.3883101867906975\n",
            "F1 Score (Weighted): 0.48567536615424406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [17:25<03:56, 118.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9\n",
            "Training loss: 0.40656900246513084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [17:35<01:58, 118.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.38266935402696783\n",
            "F1 Score (Weighted): 0.4887425676334161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 9/10 [19:24<01:58, 118.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10\n",
            "Training loss: 0.40472671381611236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [19:34<00:00, 117.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.38773051446134393\n",
            "F1 Score (Weighted): 0.4538462407609204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "print(\"\\nTraining with N_SOFT_PROMPT_TOKENS = 20\")\n",
        "\n",
        "# Load pretrained BERT model\n",
        "model_20 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG.model_name,\n",
        "    num_labels=5,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False)\n",
        "\n",
        "# Apply soft prompt tuning with OpenDelta\n",
        "soft_prompt_model_20 = SoftPromptModel(backbone_model=model_20, soft_token_num=20)\n",
        "\n",
        "for name, param in model_20.named_parameters():\n",
        "    if 'soft' not in name:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Initialize optimizer for soft prompt parameters\n",
        "\n",
        "model = model_20.to(CONFIG.device)\n",
        "optimizer_20 = AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
        "\n",
        "# Train the model\n",
        "train(model=model, optimizer=optimizer_20, train_dataloader=train_loader, val_dataloader=validation_loader)\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning"
      ],
      "metadata": {
        "id": "IbvnOeynyuki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning is the mental process of drawing conclusions, making decisions, or solving problems by thinking through information step by step. In the context of humans, it's how we make sense of things. In the context of AI and language models, it's how the model simulates a logical thought process to arrive at an answer."
      ],
      "metadata": {
        "id": "JQKGnL2cz3jW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxUrkg0u8r0W"
      },
      "source": [
        "## Chain-of-Thought (CoT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r26yOcE98r0X"
      },
      "source": [
        "LLMs have demonstrated good reasoning abilities. Furthermore, their capabilities can be further improved by incorporating reasoning techniques. One of the most notable developments in this area is the [Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903), which was introduced by Google. This approach has shown promising results in improving the reasoning capabilities of language models across a variety of tasks. Can you explain what CoT is and how it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E94KmdaIzdNj"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>\n",
        "<p dir='rtl'>زنجیره افکار (Chain-of-Thought یا CoT)، روشی است که توسط گوگل معرفی شده و به منظور ارتقای توانایی استدلال در مدل‌های زبانی بزرگ (LLM) طراحی گردیده است. این رویکرد، مدل را ترغیب می‌کند تا پیش از رسیدن به پاسخ نهایی، مجموعه‌ای از مراحل استدلال میانی را تولید کند. به جای درخواست مستقیم برای حل یک مسئله پیچیده، اعلان CoT، مدل زبانی بزرگ را هدایت می‌کند تا به صورت \"گام به گام فکر کند\"، و فرآیندی شبیه به تفکر انسانی را شبیه‌سازی نماید. هدف اصلی این تکنیک، قادر ساختن مدل به شکستن مسائل دشوار به اجزای کوچک‌تر و قابل مدیریت‌تر است تا از این طریق، دقت و قابلیت اطمینان پاسخ‌های خود را در وظایفی که نیازمند استدلال چند مرحله‌ای هستند، بهبود بخشد.</p>\n",
        "<p dir='rtl'>\n",
        "نحوه عملکرد CoT معمولاً بدین صورت است که در اعلان ورودی به مدل، چند نمونه (exemplars) ارائه می‌شود. هر نمونه شامل یک پرسش، یک فرآیند استدلال دقیق و گام به گام (همان زنجیره افکار) که به پاسخ منجر می‌شود، و در نهایت خود پاسخ است. هنگامی که پرسش جدیدی مطرح می‌شود، مدل با مشاهده این الگو، یاد می‌گیرد که نه تنها پاسخ نهایی، بلکه مراحل میانی برای رسیدن به آن را نیز تولید کند. این روش به دلیل تجزیه مسائل پیچیده به مراحل ساده‌تر، افزایش شفافیت فرآیند استدلال (با مشاهده مراحل فکری مدل) و در نتیجه بهبود قابل توجه دقت در پاسخگویی به مسائل نیازمند محاسبات، استدلال عقل سلیم و نمادین، بسیار مؤثر واقع شده است.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC13Hsv68r0X"
      },
      "source": [
        "In this section, you should use the CoT technique. firstly you need to load the [Phi-2 model](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/). This model has been introduced by Microsoft as a small LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade transformers\n",
        "%pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "Ho_o9qN1qQzS",
        "outputId": "1a9067cb-5760-44c1-a29b-50c68f7a2b79"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.28.1)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.28.1\n",
            "    Uninstalling transformers-4.28.1:\n",
            "      Successfully uninstalled transformers-4.28.1\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.52.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "c2c0002836634a8d82eb49b9dbb61ba8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZZ1v6FHF8r0Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5fee892d4f8a4170a078061f6f89c23c",
            "166138730ca24a2e9a6af87657300083",
            "e5b4641f934b4e288a4372f346f18666",
            "6de067e4778d4f5faf0e36c1e95b4dd5",
            "84d8fcad2f124dbd94d41a47fde38890",
            "aa33c8bdcacf47c19b0db164dfbc312f",
            "4f4e0248a7a64f8a8fb79d6b0feb9986",
            "55486bc165814e87926fba7a97666da5",
            "d022c8055a8e4bbcae5edfce8c1e863f",
            "147074ccb9d64c888db09c56ff94b625",
            "0786744692c940f5af1e6655f05b937e"
          ]
        },
        "outputId": "5fde5574-b23b-4fd7-8e58-b8e1ac1ccaeb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fee892d4f8a4170a078061f6f89c23c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "device = 'cuda'\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_device(device)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def generate_output(model, input, max_length=300):\n",
        "  input = f\"Question: {input}\\nOutput:\"\n",
        "  input = tokenizer(input, return_tensors=\"pt\", return_attention_mask=False)\n",
        "  input.to(device)\n",
        "  outputs = model.generate(**input, max_length=max_length)\n",
        "  text = tokenizer.batch_decode(outputs)[0]\n",
        "  return text\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ZVZVgi8r0Y"
      },
      "source": [
        "Use Phi-2 to answer the questions below with and without CoT. Compare results and explain their difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_rJQJ3VY8r0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f56c49b-0361-4ff9-f584-2bc6d575e549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "[0]\n",
            "\n",
            "**Question**\n",
            "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "\n",
            "**Answer without CoT**\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "Output: Weng earned $9 for babysitting.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[1]\n",
            "\n",
            "**Question**\n",
            "Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\n",
            "\n",
            "**Answer without CoT**\n",
            "Question: Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\n",
            "Output: To find the amount of salt in the seawater, we need to multiply the volume of water by the percentage of salt. \n",
            "\n",
            "2 liters x 20% = 0.4 liters\n",
            "\n",
            "To convert liters to milliliters, we need to multiply by 1000.\n",
            "\n",
            "0.4 liters x 1000 = 400 ml\n",
            "\n",
            "Therefore, Jack will get 400 ml of salt when all the water evaporates.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[2]\n",
            "\n",
            "**Question**\n",
            "John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\n",
            "\n",
            "**Answer without CoT**\n",
            "Question: John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\n",
            "Output: John volunteers a total of 36 hours per year (2 hours x 12 months = 24 hours per year; 24 hours x 2 months = 48 hours; 48 hours - 24 hours = 24 hours).\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[3]\n",
            "\n",
            "**Question**\n",
            "There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\n",
            "\n",
            "**Answer without CoT**\n",
            "Question: There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\n",
            "Output: There are 32 tables in the hall.\n",
            "\n",
            "Half of the tables have 2 chairs each, so there are 32/2 = 16 tables with 2 chairs each.\n",
            "\n",
            "5 tables have 3 chairs each, so there are 5 x 3 = 15 tables with 3 chairs each.\n",
            "\n",
            "The rest of the tables have 4 chairs each, so there are 32 - 16 - 5 = 11 tables with 4 chairs each.\n",
            "\n",
            "The total number of chairs in the hall is 16 x 2 + 15 x 3 + 11 x 4 = 32 + 45 + 44 = 121.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[4]\n",
            "\n",
            "**Question**\n",
            "Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\n",
            "\n",
            "**Answer without CoT**\n",
            "Question: Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\n",
            "Output: On average, it takes Bert 1050 words to use up a pencil, so he uses up a pencil every 1050/2 = 525 words.\n",
            "Since he fills out the crossword puzzle every day, he fills out a crossword puzzle every 525/7 = 75 words.\n",
            "Therefore, on average, there are 75 words in each crossword puzzle.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "questions = [\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "\"Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\",\n",
        "\"John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\",\n",
        "\"There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\",\n",
        "\"Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\"\n",
        "]\n",
        "## Correct Answers for each question:\n",
        "    # 1: $10\n",
        "    # 2: 400 ml\n",
        "    # 3: 72 hours\n",
        "    # 4: 91 chairs\n",
        "    # 5: 75 words\n",
        "\n",
        "######### Your code begins #########\n",
        "\n",
        "# Prompt the model with each question (without Chain-of-Thought) to generate direct answers\n",
        "# Correct answers for reference\n",
        "answers_without_cot = []\n",
        "for question in questions:\n",
        "    answer = generate_output(model, question)\n",
        "    answers_without_cot.append(answer)\n",
        "\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "for i in range(len(questions)):\n",
        "    print(f\"[{i}]\\n\")\n",
        "    print(\"**Question**\\n\" + questions[i])\n",
        "    print(\"\\n**Answer without CoT**\\n\" + answers_without_cot[i])\n",
        "    print(\"\\n--------------------------------------------\")\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzpx2HFgk3xX"
      },
      "source": [
        "In this part, because with the determined max length we couldn't answer the last question, in the next part, we increased max_length and again did the prompting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QOyGI7TajYPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0517e0ad-e64e-47f9-de99-9a7c3298018e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "[0]\n",
            "\n",
            "**Question**\n",
            "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "\n",
            "**Answer with CoT - Max length: 300**\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let's think step by step.\n",
            "Output: To find out how much Weng earned, we need to convert the 50 minutes into hours. Since there are 60 minutes in an hour, we divide 50 by 60 to get the decimal equivalent. 50/60 = 0.83. Now, we can multiply the decimal by Weng's hourly rate of $12. 0.83 x 12 = $9.96. Therefore, Weng earned $9.96 for babysitting.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[1]\n",
            "\n",
            "**Question**\n",
            "Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\n",
            "\n",
            "**Answer with CoT - Max length: 300**\n",
            "Question: Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates? Let's think step by step.\n",
            "Output: To find the amount of salt in the seawater, we need to multiply the volume of water by the percentage of salt. So, 2 liters x 20% = 0.4 liters of salt. To convert liters to milliliters, we need to multiply by 1000. So, 0.4 liters x 1000 = 400 ml of salt. Therefore, Jack will get 400 ml of salt when all the water evaporates.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[2]\n",
            "\n",
            "**Question**\n",
            "John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\n",
            "\n",
            "**Answer with CoT - Max length: 300**\n",
            "Question: John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year? Let's think step by step.\n",
            "Output: To determine the number of hours John volunteers per year, we need to multiply the number of hours he volunteers per month by the number of months in a year. In this case, John volunteers for 3 hours per month and there are 12 months in a year. Therefore, he volunteers for a total of 3 x 12 = 36 hours per year.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[3]\n",
            "\n",
            "**Question**\n",
            "There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\n",
            "\n",
            "**Answer with CoT - Max length: 300**\n",
            "Question: There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall? Let's think step by step.\n",
            "Output: Step 1: Find the number of tables with 2 chairs each: 32 tables / 2 = 16 tables\n",
            "Step 2: Find the number of tables with 3 chairs each: 5 tables\n",
            "Step 3: Find the number of tables with 4 chairs each: 32 tables - 16 tables - 5 tables = 11 tables\n",
            "Step 4: Find the total number of chairs: (16 tables * 2 chairs) + (5 tables * 3 chairs) + (11 tables * 4 chairs) = 32 chairs + 15 chairs + 44 chairs = 91 chairs\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[4]\n",
            "\n",
            "**Question**\n",
            "Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\n",
            "\n",
            "**Answer with CoT - Max length: 300**\n",
            "Question: Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average? Let's think step by step.\n",
            "Output: To solve this problem, we need to find the average number of words in each crossword puzzle. We know that Bert uses up a pencil every two weeks, which is equivalent to 14 days. We also know that it takes him 1050 words to use up a pencil. Therefore, we can set up a proportion:\n",
            "\n",
            "14 days / 1050 words = 1 crossword puzzle / x words\n",
            "\n",
            "To solve for x, we can cross-multiply and divide:\n",
            "\n",
            "14x = 1050\n",
            "x = 1050 / 14\n",
            "x = 75\n",
            "\n",
            "Therefore, the average number of words in each crossword puzzle is 75.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "# Use the Chain-of-Thought (CoT) prompting technique to generate step-by-step answers for each question, limiting the output length to 300 tokens\n",
        "\"\"\"Step 2: Prompting with CoT - Max length: 300\"\"\"\n",
        "answers_with_cot = []\n",
        "for question in questions:\n",
        "    question += \" Let's think step by step.\"\n",
        "    answer = generate_output(model, question,max_length=300)\n",
        "    answers_with_cot.append(answer)\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "for i in range(len(questions)):\n",
        "    print(f\"[{i}]\\n\")\n",
        "    print(\"**Question**\\n\" + questions[i])\n",
        "    print(\"\\n**Answer with CoT - Max length: 300**\\n\" + answers_with_cot[i])\n",
        "    print(\"\\n--------------------------------------------\")\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0Oc7tfq5klFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9fede0-fe74-46b9-9294-154bd02ed44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "[0]\n",
            "\n",
            "**Question**\n",
            "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "\n",
            "**Answer with CoT - Max length: 350**\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let's think step by step.\n",
            "Output: To find out how much Weng earned, we need to convert the 50 minutes into hours. Since there are 60 minutes in an hour, we divide 50 by 60 to get the decimal equivalent. 50/60 = 0.83. Now, we can multiply the decimal by Weng's hourly rate of $12. 0.83 x 12 = $9.96. Therefore, Weng earned $9.96 for babysitting.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[1]\n",
            "\n",
            "**Question**\n",
            "Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\n",
            "\n",
            "**Answer with CoT - Max length: 350**\n",
            "Question: Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates? Let's think step by step.\n",
            "Output: To find the amount of salt in the seawater, we need to multiply the volume of water by the percentage of salt. So, 2 liters x 20% = 0.4 liters of salt. To convert liters to milliliters, we need to multiply by 1000. So, 0.4 liters x 1000 = 400 ml of salt. Therefore, Jack will get 400 ml of salt when all the water evaporates.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[2]\n",
            "\n",
            "**Question**\n",
            "John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\n",
            "\n",
            "**Answer with CoT - Max length: 350**\n",
            "Question: John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year? Let's think step by step.\n",
            "Output: To determine the number of hours John volunteers per year, we need to multiply the number of hours he volunteers per month by the number of months in a year. In this case, John volunteers for 3 hours per month and there are 12 months in a year. Therefore, he volunteers for a total of 3 x 12 = 36 hours per year.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[3]\n",
            "\n",
            "**Question**\n",
            "There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\n",
            "\n",
            "**Answer with CoT - Max length: 350**\n",
            "Question: There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall? Let's think step by step.\n",
            "Output: Step 1: Find the number of tables with 2 chairs each: 32 tables / 2 = 16 tables\n",
            "Step 2: Find the number of tables with 3 chairs each: 5 tables\n",
            "Step 3: Find the number of tables with 4 chairs each: 32 tables - 16 tables - 5 tables = 11 tables\n",
            "Step 4: Find the total number of chairs: (16 tables * 2 chairs) + (5 tables * 3 chairs) + (11 tables * 4 chairs) = 32 chairs + 15 chairs + 44 chairs = 91 chairs\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n",
            "[4]\n",
            "\n",
            "**Question**\n",
            "Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\n",
            "\n",
            "**Answer with CoT - Max length: 350**\n",
            "Question: Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average? Let's think step by step.\n",
            "Output: To solve this problem, we need to find the average number of words in each crossword puzzle. We know that Bert uses up a pencil every two weeks, which is equivalent to 14 days. We also know that it takes him 1050 words to use up a pencil. Therefore, we can set up a proportion:\n",
            "\n",
            "14 days / 1050 words = 1 crossword puzzle / x words\n",
            "\n",
            "To solve for x, we can cross-multiply and divide:\n",
            "\n",
            "14x = 1050\n",
            "x = 1050 / 14\n",
            "x = 75\n",
            "\n",
            "Therefore, the average number of words in each crossword puzzle is 75.\n",
            "<|endoftext|>\n",
            "\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "\n",
        "# Use the Chain-of-Thought (CoT) prompting technique to generate step-by-step answers for each question, limiting the output length to 350 tokens\n",
        "\"\"\"Step 2: Prompting with CoT - Max length = 350\"\"\"\n",
        "answers_with_cot = []\n",
        "for question in questions:\n",
        "    question += \" Let's think step by step.\"\n",
        "    answer = generate_output(model, question,max_length=350)\n",
        "    answers_with_cot.append(answer)\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "for i in range(len(questions)):\n",
        "    print(f\"[{i}]\\n\")\n",
        "    print(\"**Question**\\n\" + questions[i])\n",
        "    print(\"\\n**Answer with CoT - Max length: 350**\\n\" + answers_with_cot[i])\n",
        "    print(\"\\n--------------------------------------------\")\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sg-W9Wz8r0Z"
      },
      "source": [
        "**Results without CoT:** Among five questions we had in our propmts, the model just answered two of them correctly while for one of them (Bert example), it followed wrong steps but it concluded to correct answer.\n",
        "\n",
        "**Results with Cot:** Cot did an incredible job and model answered all of the questions correctly. In addition, all of the reasoning step were correct and model obtained answers through correct reasoning steps. But this method has a problem, it makes llm output longer than what we had in the previous step so we will need a longer `max_length` for llm output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pknDd4gH8r0Z"
      },
      "source": [
        "## Other Methods for Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e89r-dJ8r0Z"
      },
      "source": [
        "There are many other approaches to utilize the reasoning abilities of LLMs. Describe the [Tree-of-Thought (ToT)](https://arxiv.org/abs/2305.10601) and [Self-Consistency](https://arxiv.org/abs/2203.11171) within these approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMtIiwoy8r0a"
      },
      "source": [
        " **Tree of Thoughts (ToT)**:\n",
        "   - ToT is a novel approach that enhances language model (LM) inference by allowing deliberate problem solving through interconnected reasoning steps.\n",
        "   - Key features of ToT:\n",
        "     - **Coherent Units of Text (Thoughts)**: ToT maintains a tree structure where each node represents a coherent sequence of language (a \"thought\"). These thoughts serve as intermediate steps toward solving a problem.\n",
        "     - **Self-Evaluation and Decision Making**: LMs using ToT can self-evaluate their progress by considering multiple reasoning paths. They deliberate on choices and decide the next course of action based on intermediate thoughts.\n",
        "     - **Global Choices and Backtracking**: ToT enables LMs to look ahead or backtrack when necessary, allowing for global decisions that impact the overall problem-solving process.\n",
        "\n",
        "**Self-Consistency**:\n",
        "\n",
        "  Self-consistency is an advanced prompting technique that builds on COT prompting. The aim here is to improve the naive greedy decoding using COT prompting by sampling multiple diverse reasoning paths and selecting the most consistent answers. By utilizing a majority voting system, the AI model can arrive at more accurate and reliable answers.\n",
        "\n",
        "\n",
        "  To implement self-consistency, prompt engineers typically follow these steps:\n",
        "\n",
        "- **Identify the problem:** Define the problem or question for which you require LLM's assistance. Make sure it is clear and specific.\n",
        "- **Create multiple prompts:** Develop various prompts that approach the problem from different angles or perspectives. Each prompt should provide a unique reasoning path for the AI to follow.\n",
        "- **Generate responses:** Submit the prompts to LLM and obtain the responses generated by the model.\n",
        "- **Evaluate consistency:** Analyze the generated responses to determine their coherence, relevance, and consistency. This step may involve comparing the responses to each other, looking for common themes or patterns, and checking for internal logical consistency.\n",
        "- **Select the best response:** Based on the evaluation, choose the most consistent and accurate response as the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j3GS92m8r0a"
      },
      "source": [
        "Now, implement Self-Consistency to answer the questions of the previous section.\n",
        "Analyze the results obtained from Steps 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HzIqzHWq43hR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb99666a-8087-417a-b976-382ba6168dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "Step 3: Prompting with CoT and Self-Consistency - Max length = 350\n",
            "--------------------------------------------\n",
            "[0]\n",
            "**Question**\n",
            "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
            "\n",
            "**Responses from Different Prompts**\n",
            "Prompt 1:  Let's think step by step to solve this problem.\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let's think step by step to solve this problem.\n",
            "Output: To find out how much Weng earned, we need to calculate her earnings per minute and then multiply it by the number of minutes she babysat.\n",
            "\n",
            "Step 1: Calculate Weng's earnings per minute.\n",
            "Weng earns $12 an hour, which is equivalent to $12/60 = $0.20 per minute.\n",
            "\n",
            "Step 2: Multiply her earnings per minute by the number of minutes she babysat.\n",
            "Weng babysat for 50 minutes, so her earnings would be $0.20/minute * 50 minutes = $10.\n",
            "\n",
            "Therefore, Weng earned $10 for babysitting.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 2:  Break down the problem into clear steps and explain each one.\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Break down the problem into clear steps and explain each one.\n",
            "Output: Step 1: Convert the minutes into hours. Since there are 60 minutes in an hour, 50 minutes is equal to 50/60 = 5/6 hours.\n",
            "Step 2: Multiply the number of hours by the hourly rate. 5/6 hours * $12/hour = $10.\n",
            "Step 3: Weng earned $10 for babysitting.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 3:  Solve this problem systematically, showing all calculations step by step.\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Solve this problem systematically, showing all calculations step by step.\n",
            "Output: First, convert the minutes into hours. 50 minutes is 50/60 = 0.83 hours. Then, multiply the hours by the hourly rate. 0.83 * $12 = $10.04. Therefore, Weng earned $10.04.\n",
            "<|endoftext|>\n",
            "\n",
            "**Final Answer with Self-Consistency**\n",
            "12 \n",
            "**Correct Answer**: $10\n",
            "\n",
            "--------------------------------------------\n",
            "[1]\n",
            "**Question**\n",
            "Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates?\n",
            "\n",
            "**Responses from Different Prompts**\n",
            "Prompt 1:  Let's think step by step to solve this problem.\n",
            "Question: Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates? Let's think step by step to solve this problem.\n",
            "Output: To find the amount of salt in the seawater, we need to multiply the volume of water by the percentage of salt. This gives us 2 liters x 20% = 0.4 liters of salt. To convert liters to milliliters, we need to multiply by 1000. This gives us 0.4 liters x 1000 = 400 ml of salt. Therefore, Jack will get 400 ml of salt when all the water evaporates.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 2:  Break down the problem into clear steps and explain each one.\n",
            "Question: Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates? Break down the problem into clear steps and explain each one.\n",
            "Output: Step 1: Convert the volume of seawater from liters to milliliters. 2 liters = 2000 ml.\n",
            "Step 2: Calculate the amount of salt in the seawater. 20% of 2000 ml = 0.2 * 2000 = 400 ml.\n",
            "Step 3: Jack will get 400 ml of salt when all the water evaporates.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 3:  Solve this problem systematically, showing all calculations step by step.\n",
            "Question: Jack is stranded on a desert island. He wants some salt to season his fish. He collects 2 liters of seawater in an old bucket. If the water is 20% salt, how many ml of salt will Jack get when all the water evaporates? Solve this problem systematically, showing all calculations step by step.\n",
            "Output: \n",
            "Step 1: Convert the volume of seawater from liters to milliliters.\n",
            "2 liters = 2000 ml\n",
            "\n",
            "Step 2: Calculate the amount of salt in the seawater.\n",
            "20% of 2000 ml = 0.20 * 2000 = 400 ml\n",
            "\n",
            "So, Jack will get 400 ml of salt when all the water evaporates.\n",
            "<|endoftext|>\n",
            "\n",
            "**Final Answer with Self-Consistency**\n",
            "2 \n",
            "**Correct Answer**: 400 ml\n",
            "\n",
            "--------------------------------------------\n",
            "[2]\n",
            "**Question**\n",
            "John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year?\n",
            "\n",
            "**Responses from Different Prompts**\n",
            "Prompt 1:  Let's think step by step to solve this problem.\n",
            "Question: John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year? Let's think step by step to solve this problem.\n",
            "Output: To find the total number of hours John volunteers per year, we need to multiply the number of hours he volunteers per month by the number of months in a year.\n",
            "\n",
            "Number of hours John volunteers per month = 2 hours\n",
            "Number of months in a year = 12 months\n",
            "\n",
            "Total number of hours John volunteers per year = 2 hours/month * 12 months/year = 24 hours/year\n",
            "\n",
            "Therefore, John volunteers for 24 hours per year.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 2:  Break down the problem into clear steps and explain each one.\n",
            "Question: John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year? Break down the problem into clear steps and explain each one.\n",
            "Output: Step 1: Calculate the number of hours John volunteers per month. 2 times a month * 3 hours = 6 hours per month.\n",
            "Step 2: Calculate the number of hours John volunteers per year. 6 hours per month * 12 months = 72 hours per year.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 3:  Solve this problem systematically, showing all calculations step by step.\n",
            "Question: John volunteers at a shelter twice a month for 3 hours at a time. How many hours does he volunteer per year? Solve this problem systematically, showing all calculations step by step.\n",
            "Output: To solve this problem, we need to calculate the total number of hours John volunteers per year.\n",
            "\n",
            "Step 1: Calculate the number of hours John volunteers per month.\n",
            "John volunteers twice a month for 3 hours at a time.\n",
            "Number of hours per month = 2 * 3 = 6 hours\n",
            "\n",
            "Step 2: Calculate the number of hours John volunteers per year.\n",
            "Number of hours per year = Number of hours per month * 12 months\n",
            "Number of hours per year = 6 * 12 = 72 hours\n",
            "\n",
            "Therefore, John volunteers 72 hours per year.\n",
            "<|endoftext|>\n",
            "\n",
            "**Final Answer with Self-Consistency**\n",
            "3 hours\n",
            "**Correct Answer**: 72 hours\n",
            "\n",
            "--------------------------------------------\n",
            "[3]\n",
            "**Question**\n",
            "There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall?\n",
            "\n",
            "**Responses from Different Prompts**\n",
            "Prompt 1:  Let's think step by step to solve this problem.\n",
            "Question: There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall? Let's think step by step to solve this problem.\n",
            "Output: Step 1: Find the number of tables with 2 chairs each.\n",
            "Half the tables have 2 chairs each, so there are 32/2 = 16 tables with 2 chairs each.\n",
            "\n",
            "Step 2: Find the number of tables with 3 chairs each.\n",
            "There are 5 tables with 3 chairs each.\n",
            "\n",
            "Step 3: Find the number of tables with 4 chairs each.\n",
            "The remaining tables have 4 chairs each, so there are 32 - 16 - 5 = 11 tables with 4 chairs each.\n",
            "\n",
            "Step 4: Find the total number of chairs.\n",
            "The total number of chairs is (16 * 2) + (5 * 3) + (11 * 4) = 32 + 15 + 44 = 91 chairs.\n",
            "\n",
            "Therefore, there are 91 chairs in total in the hall.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 2:  Break down the problem into clear steps and explain each one.\n",
            "Question: There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall? Break down the problem into clear steps and explain each one.\n",
            "Output: Step 1: Find the number of tables with 2 chairs each.\n",
            "Half the tables have 2 chairs each, so there are 32/2 = 16 tables with 2 chairs each.\n",
            "\n",
            "Step 2: Find the number of tables with 3 chairs each.\n",
            "There are 5 tables with 3 chairs each.\n",
            "\n",
            "Step 3: Find the number of tables with 4 chairs each.\n",
            "The rest of the tables have 4 chairs each, so there are 32 - 16 - 5 = 11 tables with 4 chairs each.\n",
            "\n",
            "Step 4: Calculate the total number of chairs.\n",
            "The total number of chairs is (16 tables with 2 chairs each * 2 chairs) + (5 tables with 3 chairs each * 3 chairs) + (11 tables with 4 chairs each * 4 chairs) = 32 + 15 + 44 = 91 chairs.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 3:  Solve this problem systematically, showing all calculations step by step.\n",
            "Question: There are 32 tables in a hall. Half the tables have 2 chairs each, 5 have 3 chairs each and the rest have 4 chairs each. How many chairs in total are in the hall? Solve this problem systematically, showing all calculations step by step.\n",
            "Output: Step 1: Find the number of tables with 2 chairs each.\n",
            "Half the tables have 2 chairs each, so there are 32/2 = 16 tables with 2 chairs each.\n",
            "\n",
            "Step 2: Find the number of tables with 3 chairs each.\n",
            "There are 5 tables with 3 chairs each.\n",
            "\n",
            "Step 3: Find the number of tables with 4 chairs each.\n",
            "The remaining tables are 32 - 16 - 5 = 11 tables with 4 chairs each.\n",
            "\n",
            "Step 4: Calculate the total number of chairs.\n",
            "The total number of chairs is (16 tables * 2 chairs) + (5 tables * 3 chairs) + (11 tables * 4 chairs) = 32 + 15 + 44 = 91 chairs.\n",
            "<|endoftext|>\n",
            "\n",
            "**Final Answer with Self-Consistency**\n",
            "32 \n",
            "**Correct Answer**: 91 chairs\n",
            "\n",
            "--------------------------------------------\n",
            "[4]\n",
            "**Question**\n",
            "Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average?\n",
            "\n",
            "**Responses from Different Prompts**\n",
            "Prompt 1:  Let's think step by step to solve this problem.\n",
            "Question: Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average? Let's think step by step to solve this problem.\n",
            "Output: To find the average number of words in each crossword puzzle, we need to divide the total number of words used up by the number of puzzles filled out. \n",
            "First, we need to find the number of puzzles filled out in two weeks. Since Bert fills out the puzzle every day, in two weeks he would have filled out 14 puzzles. \n",
            "Next, we need to find the total number of words used up in two weeks. Since it takes 1050 words to use up a pencil, in two weeks Bert would have used up 1050 x 14 = 14,700 words. \n",
            "Finally, we can find the average number of words in each crossword puzzle by dividing the total number of words used up by the number of puzzles filled out. \n",
            "Therefore, the average number of words in each crossword puzzle is 14,700 / 14 = 1,050 words.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 2:  Break down the problem into clear steps and explain each one.\n",
            "Question: Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average? Break down the problem into clear steps and explain each one.\n",
            "Output: Step 1: Determine how many pencils Bert uses in a year. Since he uses a pencil every two weeks, he uses a pencil 52/2 = 26 times a year.\n",
            "Step 2: Determine how many words Bert uses in a year. Since it takes 1050 words to use up a pencil, he uses 1050 * 26 = 27,300 words in a year.\n",
            "Step 3: Determine how many words are in each crossword puzzle. To do this, divide the total number of words Bert uses in a year by the number of crossword puzzles he fills out in a year. Since he fills out one crossword puzzle every day, he fills out 365 crossword puzzles in a year. So, the average number of words in each crossword puzzle is 27,300 / 365 = 75.\n",
            "<|endoftext|>\n",
            "\n",
            "Prompt 3:  Solve this problem systematically, showing all calculations step by step.\n",
            "Question: Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average? Solve this problem systematically, showing all calculations step by step.\n",
            "Output: Let's first find out how many pencils Bert uses in a year. There are 52 weeks in a year, and he uses a pencil every two weeks. So, he uses 52/2 = 26 pencils in a year.\n",
            "\n",
            "Next, let's find out how many words he uses in a year. It takes him 1050 words to use up a pencil, so he uses 1050 * 26 = 27,300 words in a year.\n",
            "\n",
            "Finally, let's find out how many words are in each crossword puzzle on average. He fills out the crossword puzzle every day, so there are 365 days in a year. So, he fills out 365 puzzles in a year. Therefore, on average, there are 27,300 / 365 = 75.8 words in each crossword puzzle.\n",
            "<|endoftext|>\n",
            "\n",
            "**Final Answer with Self-Consistency**\n",
            "1050 words\n",
            "**Correct Answer**: 75 words\n",
            "\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "######### Your code begins #########\n",
        "from collections import Counter\n",
        "# Apply multiple Chain-of-Thought (CoT) prompts with self-consistency to generate answers and compare the results across different prompt variations\n",
        "# **** Question: diff prompts? we should recognize it by ourslves or not\n",
        "\"\"\"Step 3: Prompting with CoT and Self-Consistency - Max length = 350\"\"\"\n",
        "\n",
        "\n",
        "# Correct answers for reference\n",
        "correct_answers = [\"$10\", \"400 ml\", \"72 hours\", \"91 chairs\", \"75 words\"]\n",
        "\n",
        "# Define multiple CoT prompt variations to encourage diverse reasoning\n",
        "prompt_variations = [\n",
        "    \" Let's think step by step to solve this problem.\",\n",
        "    \" Break down the problem into clear steps and explain each one.\",\n",
        "    \" Solve this problem systematically, showing all calculations step by step.\"\n",
        "]\n",
        "\n",
        "# Implement Self-Consistency\n",
        "answers_with_self_consistency = []\n",
        "for question in questions:\n",
        "    responses = []\n",
        "    for prompt in prompt_variations:\n",
        "        # Generate response for each prompt variation\n",
        "        modified_question = question + prompt\n",
        "        response = generate_output(model, modified_question, max_length=350)\n",
        "        responses.append(response)\n",
        "\n",
        "    # Extract numerical answers using regex\n",
        "    numerical_answers = []\n",
        "    for response in responses:\n",
        "        # Look for numbers with units or currency symbols where applicable\n",
        "        match = re.search(r'\\b\\d+(\\.\\d+)?\\s*(?:\\$|ml|hours?|chairs?|words?)?\\b', response)\n",
        "        if match:\n",
        "            numerical_answers.append(match.group(0))\n",
        "\n",
        "    # Apply majority voting\n",
        "    if numerical_answers:\n",
        "        answer_counts = Counter(numerical_answers)\n",
        "        most_common_answer = answer_counts.most_common(1)[0][0]\n",
        "    else:\n",
        "        most_common_answer = \"No clear numerical answer found\"\n",
        "\n",
        "    answers_with_self_consistency.append({\n",
        "        \"question\": question,\n",
        "        \"responses\": responses,\n",
        "        \"final_answer\": most_common_answer\n",
        "    })\n",
        "\n",
        "# Print results for comparison\n",
        "print(\"--------------------------------------------\")\n",
        "print(\"Step 3: Prompting with CoT and Self-Consistency - Max length = 350\")\n",
        "print(\"--------------------------------------------\")\n",
        "for i, result in enumerate(answers_with_self_consistency):\n",
        "    print(f\"[{i}]\")\n",
        "    print(\"**Question**\")\n",
        "    print(result[\"question\"])\n",
        "    print(\"\\n**Responses from Different Prompts**\")\n",
        "    for j, response in enumerate(result[\"responses\"]):\n",
        "        print(f\"Prompt {j+1}: {prompt_variations[j]}\")\n",
        "        print(response)\n",
        "        print()\n",
        "    print(\"**Final Answer with Self-Consistency**\")\n",
        "    print(result[\"final_answer\"])\n",
        "    print(f\"**Correct Answer**: {correct_answers[i]}\")\n",
        "    print(\"\\n--------------------------------------------\")\n",
        "\n",
        "######### Your code ends ###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S72rMyRB8r0a"
      },
      "source": [
        "Consider LLMs' features and propose a new approach based on them to enhance LLMs' reasoning abilities. Why do you believe this approach could enhance LLMs' reasoning abilities?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA5u6MoiSsRC"
      },
      "source": [
        "<font color='#FA5170'><b>ِYour answer:</b></font>\n",
        "<p dir='rtl'>یک رویکرد نوین برای تقویت استدلال مدل‌های زبانی بزرگ می‌تواند «تأمل و پالایش ساختاریافته تکرارشونده» نام گیرد. این روش با بهره‌گیری از ویژگی‌های بنیادین این مدل‌ها نظیر دانش گسترده و توانایی یادگیری در زمینه (in-context learning)، مدل را ابتدا به تجزیه مسئله به اجزای کوچک‌تر و سپس به استخراج صریح دانش مرتبط با هر جزء وامی‌دارد. در ادامه، مدل به تولید چندین مسیر استدلالی، ارزیابی منتقدانه هر مسیر با شناسایی نقاط ضعف و قوت، و تخصیص امتیاز اطمینان می‌پردازد و این فرآیند را به صورت تکراری برای بهبود و انتخاب بهترین مسیر ادامه می‌دهد تا در نهایت پاسخ جامع را ارائه کند. باور بر این است که چنین رویکردی با ترویج تحلیل عمیق‌تر، ملزم ساختن مدل به بررسی نظام‌مند خطاها، و تقویت اتکای استدلال به پایگاه دانش موجود، به شکل قابل توجهی توانایی استدلال منطقی و دقت مدل در مواجهه با مسائل پیچیده را ارتقا می‌بخشد.</p>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7xpsgvYumvNa",
        "qmviyjCrz6mi"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fee892d4f8a4170a078061f6f89c23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_166138730ca24a2e9a6af87657300083",
              "IPY_MODEL_e5b4641f934b4e288a4372f346f18666",
              "IPY_MODEL_6de067e4778d4f5faf0e36c1e95b4dd5"
            ],
            "layout": "IPY_MODEL_84d8fcad2f124dbd94d41a47fde38890"
          }
        },
        "166138730ca24a2e9a6af87657300083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa33c8bdcacf47c19b0db164dfbc312f",
            "placeholder": "​",
            "style": "IPY_MODEL_4f4e0248a7a64f8a8fb79d6b0feb9986",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e5b4641f934b4e288a4372f346f18666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55486bc165814e87926fba7a97666da5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d022c8055a8e4bbcae5edfce8c1e863f",
            "value": 2
          }
        },
        "6de067e4778d4f5faf0e36c1e95b4dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_147074ccb9d64c888db09c56ff94b625",
            "placeholder": "​",
            "style": "IPY_MODEL_0786744692c940f5af1e6655f05b937e",
            "value": " 2/2 [00:23&lt;00:00, 10.08s/it]"
          }
        },
        "84d8fcad2f124dbd94d41a47fde38890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa33c8bdcacf47c19b0db164dfbc312f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4e0248a7a64f8a8fb79d6b0feb9986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55486bc165814e87926fba7a97666da5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d022c8055a8e4bbcae5edfce8c1e863f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "147074ccb9d64c888db09c56ff94b625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0786744692c940f5af1e6655f05b937e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}